{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will first try to train and optimize some ML algorithms to predict the survival of individuals, using all available and meaningful data, \n",
    "\n",
    "### and then, following the data analysis, we search for the best algorithm only for predicting the survival of males, following the schema:\n",
    "- if female:\n",
    "    - if all family died:\n",
    "        - predict die\n",
    "    - else predict survive\n",
    "- if male:\n",
    "    - if all family survives:\n",
    "        - predict survive\n",
    "    - else predict survival using ML on data: IsYoung, PClass, and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"CSV\", \"DataFrames\", \"Statistics\", \"MLJ\", \"MLJFlux\", \"Flux\", \"MLJLIBSVMInterface\", \"NearestNeighborModels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, MLJ, MLJFlux, Flux, MLJLIBSVMInterface, NearestNeighborModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.File(\"data/train.csv\") |> DataFrame\n",
    "X_pred = CSV.File(\"data/test.csv\") |> DataFrame;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a pipeline for pre-processing the data\n",
    "The columns we will use are:\n",
    "* Sex\n",
    "* IsYoung / CategoricalAge\n",
    "* Pclass\n",
    "* Embarked\n",
    "* FamilySurvived\n",
    "\n",
    "We will also have to change the scientific type of our data for it to work with the Julia ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "port_to_numerical (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function encodes the port name\n",
    "function port_to_numerical(port)\n",
    "    if ismissing(port) || port == \"S\"\n",
    "        return 0 \n",
    "    elseif port == \"C\"\n",
    "        return 2\n",
    "    else\n",
    "        return 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_family_dict (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex to extract first name\n",
    "family_name_regex = r\"^([\\w\\-]+)\"\n",
    "\n",
    "# Function returns a dictionary containing family names as keys, \n",
    "# and an array with the \"Survived\" label for all family members as value \n",
    "function get_family_dict(data)\n",
    "    family_dict = Dict()\n",
    "    \n",
    "    # extract string\n",
    "    for value in eachrow(data)\n",
    "        name = match(family_name_regex, value.Name).match\n",
    "        if haskey(family_dict, name)\n",
    "            # push survived value to set matching the name \n",
    "            push!(get(family_dict, name, nothing), value.Survived)\n",
    "        else\n",
    "            # if dict does not contain this family, create a new set containing the \"Survived\" value of this person\n",
    "            family_dict[name] = [value.Survived]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return family_dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "family_survived (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function returns 0 if all family died\n",
    "# - 1 if the person is travelling alone or family has mixed survival\n",
    "# - 2 if all family survived\n",
    "function family_survived(example, family_dict)\n",
    "    name = match(family_name_regex, example.Name).match\n",
    "    \n",
    "    if haskey(family_dict, name) && length(get(family_dict, name, nothing)) > 1\n",
    "        value = get(family_dict, name, nothing)\n",
    "        if mean(value) == 1\n",
    "            # all family survived\n",
    "            return 2\n",
    "        elseif mean(value) == 0\n",
    "            # all family died\n",
    "            return 0\n",
    "        end\n",
    "        # mixed survival\n",
    "        return 1\n",
    "    else\n",
    "        # person is travelling alone\n",
    "        return 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pre_process (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for pre-processing the data\n",
    "# If given training data, only the \"data\" parameter has to be speciffied.\n",
    "# For data to be predicted, the family dictionary must also be passed.\n",
    "# @return family_dict only for training data.\n",
    "function pre_process(data; train=true, family_dict=missing)\n",
    "    # Sex to numerical\n",
    "    data.Sex = map(x -> x == \"male\" ? 0 : 1, data.Sex)\n",
    "    \n",
    "    # IsYoung / Age\n",
    "    # TODO\n",
    "    data.Age = map(x -> !ismissing(x) && x < 14 ? 1 : 0, data.Age)\n",
    "    rename!(data, :Age => :IsYoung)\n",
    "    \n",
    "    # Embarked to numerical\n",
    "    data.Embarked = map(x -> port_to_numerical(x), data.Embarked)\n",
    "    \n",
    "    # FamilySurvived\n",
    "    if train\n",
    "        family_dict = get_family_dict(data)\n",
    "    end\n",
    "    data.FamilySurvived = map(x -> family_survived(x, family_dict), eachrow(data))\n",
    "        \n",
    "    # drop cols\n",
    "    select!(data, Not([:PassengerId, :Name, :SibSp, :Parch, :Fare, :Ticket, :Cabin]))\n",
    "    \n",
    "    # scitype\n",
    "    coerce!(data, Count => Continuous)\n",
    "    if train\n",
    "        coerce!(data, :Survived => OrderedFactor)\n",
    "        return family_dict\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_dict = pre_process(data)\n",
    "pre_process(X_pred, train=false, family_dict=family_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data now is in the right format. The next step is to break it into examples and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(data, ==(:Survived), x->true, rng=123);\n",
    "# shuffle data with a seed to mentain a certain consistency between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(eachindex(y), 0.7, shuffle=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find models that can be fitted to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_to_file (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function write_to_file(data, path::String)\n",
    "    fw = open(path, \"w\")\n",
    "    write(fw, \"PassengerId,Survived\\n\")\n",
    "    close(fw)\n",
    "    fw = open(path, \"a\")\n",
    "    index = 892\n",
    "    for value in data\n",
    "        write(fw, \"$index,$value\\n\")\n",
    "        index += 1\n",
    "    end\n",
    "    close(fw)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks time :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the best results, we must first normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standardizer(\n",
       "    features = Symbol[],\n",
       "    ignore = false,\n",
       "    ordered_factor = false,\n",
       "    count = false)\u001b[34m @704\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand = Standardizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n",
      "┌ Warning: `acceleration isa CUDALibs` but no CUDA device (GPU) currently live. \n",
      "└ @ MLJFlux /home/ahautelman/.julia/packages/MLJFlux/AeMUx/src/classifier.jl:39\n"
     ]
    }
   ],
   "source": [
    "Model = @load NeuralNetworkClassifier\n",
    "# we'll use mini-batch for training the MLP for faster convergence\n",
    "# also change the number of hidden neurons from the default 0 to 10 \n",
    "#     to allow the network to learn the, possibly, more complex relations between features. \n",
    "model = Model(builder=MLJFlux.Short(n_hidden=10, σ=Flux.elu, dropout=0.2),\n",
    "    epochs=200, batch_size=100,\n",
    "    acceleration=CUDALibs());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = @pipeline stand model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NumericRange)(Float64, :(neural_network_classifier.builder.dropout), ... )"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define some ranges for the hyper-parameters that we want to optimize.\n",
    "r = range(pipe, :(neural_network_classifier.lambda), lower=0.0, upper=10.0)\n",
    "r2 = range(pipe, :(neural_network_classifier.alpha), lower=0.0, upper=4.0)\n",
    "r3 = range(pipe, :(neural_network_classifier.builder.dropout), lower=0, upper=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedModel(model=pipe,\n",
    "                   ranges = [r, r2, r3],\n",
    "                   resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   repeats=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @865\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 1000 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 1000 metamodels: 100%[=========================] Time: 0:35:50\u001b[39m\n",
      "┌ Warning: `acceleration isa CUDALibs` but no CUDA device (GPU) currently live. \n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @865\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @800\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @456\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)\n",
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14606741573033707"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, X[test, :]);\n",
    "misclassification_rate(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good score on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = convert(Array{Int64}, mode.(predict(mach, X_pred)));\n",
    "write_to_file(prediction, \"data/nn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle score: 0.78708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"NN.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLIBSVMInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(\n",
       "    kernel = LIBSVM.Kernel.RadialBasis,\n",
       "    gamma = 0.0,\n",
       "    weights = nothing,\n",
       "    cost = 1.0,\n",
       "    cachesize = 200.0,\n",
       "    degree = 3,\n",
       "    coef0 = 0.0,\n",
       "    tolerance = 0.001,\n",
       "    shrinking = true,\n",
       "    probability = false)\u001b[34m @564\u001b[39m"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load SVC\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = @pipeline stand model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{Pipeline423,…} @698\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @191\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @996\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{Pipeline423,…} @698\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Training \u001b[34mMachine{Standardizer,…} @476\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Training \u001b[34mMachine{SVC,…} @949\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{Pipeline423,…} @698\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @191\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @996\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13108614232209737"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, X[test, :])\n",
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even better score for the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = convert(Array{Int64}, predict(mach, X_pred))\n",
    "write_to_file(prediction, \"data/svc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle score: 0.78468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"SVC.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = -1,\n",
       "    n_trees = 10,\n",
       "    sampling_fraction = 0.7,\n",
       "    pdf_smoothing = 0.0)\u001b[34m @446\u001b[39m"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = @load RandomForestClassifier pkg=\"DecisionTree\"\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = range(model, :min_samples_leaf, lower=1, upper=20)\n",
    "r3 = range(model, :min_samples_split, lower=1, upper=40)\n",
    "r4 = range(model, :n_subfeatures, lower=0, upper=5)\n",
    "r5 = range(model, :n_trees, lower=2, upper=15)\n",
    "r6 = range(model, :sampling_fraction, lower=0.5, upper=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = RandomForestClassifier(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 1,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = -1,\n",
       "            n_trees = 10,\n",
       "            sampling_fraction = 0.7,\n",
       "            pdf_smoothing = 0.0),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = Holdout(\n",
       "            fraction_train = 0.8,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = LogLoss(\n",
       "            tol = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[\u001b[34mNumericRange{Int64,…} @286\u001b[39m, \u001b[34mNumericRange{Int64,…} @121\u001b[39m, \u001b[34mNumericRange{Int64,…} @141\u001b[39m, \u001b[34mNumericRange{Int64,…} @883\u001b[39m, \u001b[34mNumericRange{Float64,…} @909\u001b[39m],\n",
       "    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "    train_best = true,\n",
       "    repeats = 5,\n",
       "    n = 100000,\n",
       "    acceleration = CPUProcesses{Nothing}(nothing),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true,\n",
       "    cache = true)\u001b[34m @531\u001b[39m"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TunedModel(model=model,\n",
    "                  ranges=[r2, r3, r4, r5, r6],\n",
    "                  resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                  measures=cross_entropy,\n",
    "                  repeats=5,\n",
    "                  n=100000,\n",
    "                  acceleration=CPUProcesses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(model, X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @842\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 100000 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 60000 metamodels: 100%[=========================] Time: 0:07:32\u001b[39m\n",
      "┌ Info: Only 60000 (of 100000) models evaluated.\n",
      "│ Model supply exhausted. \n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @842\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @112\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @544\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18352059925093633"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(predict(mach, X[test, :]))\n",
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = convert(Array{Int64}, mode.(predict(mach, X_pred)))\n",
    "write_to_file(prediction, \"data/random_forest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle score: 0.77751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"random_forest.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN clasiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import NearestNeighborModels ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/E8BbE/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = Uniform())\u001b[34m @346\u001b[39m"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KNNClassifier pkg=\"NearestNeighborModels\"\n",
    "model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline447(\n",
       "    standardizer = Standardizer(\n",
       "            features = Symbol[],\n",
       "            ignore = false,\n",
       "            ordered_factor = false,\n",
       "            count = false),\n",
       "    knn_classifier = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = Uniform()))\u001b[34m @015\u001b[39m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = @pipeline stand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NominalRange)(Int64, :(knn_classifier.K), ... )"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(pipe, :(knn_classifier.K), values=2:40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedModel(model=pipe,\n",
    "                   ranges=r,\n",
    "                   resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   repeats=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @122\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @249\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @462\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @122\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 39 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/7mI9k/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 39 metamodels: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @122\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @249\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @462\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12734082397003746"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(MLJ.predict(mach, X[test, :]))\n",
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = convert(Array{Int64}, mode.(predict(mach, X_pred)))\n",
    "write_to_file(prediction, \"data/knn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle score: 0.78947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"knn.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light gbm ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: use other approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have seen, the algorithms are all consistent and have good results.\n",
    "But we got an accuracy increase of only 2% compared to the naive 'all women survive' prediction.\n",
    "\n",
    "To change the tactics a bit, we will go by this next schema in the steps to come:\n",
    "- if female:\n",
    "    - if all family died:\n",
    "        - predict die\n",
    "    - else predict survival using ML\n",
    "- if male:\n",
    "    - if all family survives:\n",
    "        - predict survive\n",
    "    - else predict survival using ML algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/E8BbE/src/loading.jl:168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import NearestNeighborModels ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = Uniform())\u001b[34m @395\u001b[39m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KNNClassifier pkg=\"NearestNeighborModels\"\n",
    "model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NominalRange)(Int64, :K, ... )"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(model, :K, values=2:40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedModel(model=model,\n",
    "                   ranges=r,\n",
    "                   resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   repeats=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @802\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 39 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/7mI9k/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 39 metamodels: 100%[=========================] Time: 0:00:19\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @802\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @497\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @529\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)\n",
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_predict (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_predict(data, mach)\n",
    "    ŷ = []\n",
    "    for value in eachrow(data)\n",
    "        push!(ŷ, predict_example(value, mach))\n",
    "    end\n",
    "    return ŷ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_example (generic function with 1 method)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict_example(example, mach)\n",
    "    if example.FamilySurvived == 0\n",
    "        return 0    \n",
    "    elseif example.FamilySurvived == 2\n",
    "        return 1\n",
    "    else\n",
    "        pred = predict(mach, DataFrame(example))\n",
    "        # have to do this dumb shit cus julia has problems with predicting only 1 example\n",
    "        for d in pred\n",
    "            if pdf(d, 0.0) >= 0.5\n",
    "                return 0\n",
    "            else\n",
    "                return 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418-element Vector{Any}:\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = my_predict(X_pred, mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(prediction, \"knn_stack.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "pdf(d::UnivariateDistribution, x::Real)\n",
       "\\end{verbatim}\n",
       "Evaluate the probability density (mass) at \\texttt{x}.\n",
       "\n",
       "See also: \\href{@ref}{\\texttt{logpdf}}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "pdf(d::MultivariateDistribution, x::AbstractArray)\n",
       "\\end{verbatim}\n",
       "Return the probability density of distribution \\texttt{d} evaluated at \\texttt{x}.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item If \\texttt{x} is a vector, it returns the result as a scalar.\n",
       "\n",
       "\n",
       "\\item If \\texttt{x} is a matrix with n columns, it returns a vector \\texttt{r} of length n, where \\texttt{r[i]} corresponds\n",
       "\n",
       "\\end{itemize}\n",
       "to \\texttt{x[:,i]} (i.e. treating each column as a sample).\n",
       "\n",
       "\\texttt{pdf!(r, d, x)} will write the results to a pre-allocated array \\texttt{r}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "pdf(d::MatrixDistribution, x::AbstractArray)\n",
       "\\end{verbatim}\n",
       "Compute the probability density at the input matrix \\texttt{x}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "pdf(d::Union{UnivariateMixture, MultivariateMixture}, x)\n",
       "\\end{verbatim}\n",
       "Evaluate the (mixed) probability density function over \\texttt{x}. Here, \\texttt{x} can be a single sample or an array of multiple samples.\n",
       "\n",
       "\\begin{verbatim}\n",
       "Distributions.pdf(d::UnivariateFinite, x)\n",
       "\\end{verbatim}\n",
       "Probability of \\texttt{d} at \\texttt{x}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "v = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\n",
       "d = UnivariateFinite(v[1:2], [0.3, 0.7])\n",
       "pdf(d, \"yes\")     # 0.3\n",
       "pdf(d, v[1])      # 0.3\n",
       "pdf(d, \"no\")      # 0.0\n",
       "pdf(d, \"house\")   # throws error\n",
       "\\end{verbatim}\n",
       "Other similar methods are available too:\n",
       "\n",
       "\\begin{verbatim}\n",
       "mode(d)    # CategoricalValue{String, UInt32} \"maybe\"\n",
       "rand(d, 5) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"] or similar\n",
       "d = fit(UnivariateFinite, v)\n",
       "pdf(d, \"maybe\") # 0.25\n",
       "logpdf(d, \"maybe\") # log(0.25)\n",
       "\\end{verbatim}\n",
       "One can also do weighted fits:\n",
       "\n",
       "\\begin{verbatim}\n",
       "w = [1, 4, 5, 1] # some weights\n",
       "d = fit(UnivariateFinite, v, w)\n",
       "pdf(d, \"maybe\") ≈ 4/11 # true\n",
       "\\end{verbatim}\n",
       "See also \\texttt{classes}, \\texttt{support}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "pdf(d::UnivariateDistribution, x::Real)\n",
       "```\n",
       "\n",
       "Evaluate the probability density (mass) at `x`.\n",
       "\n",
       "See also: [`logpdf`](@ref).\n",
       "\n",
       "```\n",
       "pdf(d::MultivariateDistribution, x::AbstractArray)\n",
       "```\n",
       "\n",
       "Return the probability density of distribution `d` evaluated at `x`.\n",
       "\n",
       "  * If `x` is a vector, it returns the result as a scalar.\n",
       "  * If `x` is a matrix with n columns, it returns a vector `r` of length n, where `r[i]` corresponds\n",
       "\n",
       "to `x[:,i]` (i.e. treating each column as a sample).\n",
       "\n",
       "`pdf!(r, d, x)` will write the results to a pre-allocated array `r`.\n",
       "\n",
       "```\n",
       "pdf(d::MatrixDistribution, x::AbstractArray)\n",
       "```\n",
       "\n",
       "Compute the probability density at the input matrix `x`.\n",
       "\n",
       "```\n",
       "pdf(d::Union{UnivariateMixture, MultivariateMixture}, x)\n",
       "```\n",
       "\n",
       "Evaluate the (mixed) probability density function over `x`. Here, `x` can be a single sample or an array of multiple samples.\n",
       "\n",
       "```\n",
       "Distributions.pdf(d::UnivariateFinite, x)\n",
       "```\n",
       "\n",
       "Probability of `d` at `x`.\n",
       "\n",
       "```\n",
       "v = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\n",
       "d = UnivariateFinite(v[1:2], [0.3, 0.7])\n",
       "pdf(d, \"yes\")     # 0.3\n",
       "pdf(d, v[1])      # 0.3\n",
       "pdf(d, \"no\")      # 0.0\n",
       "pdf(d, \"house\")   # throws error\n",
       "```\n",
       "\n",
       "Other similar methods are available too:\n",
       "\n",
       "```\n",
       "mode(d)    # CategoricalValue{String, UInt32} \"maybe\"\n",
       "rand(d, 5) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"] or similar\n",
       "d = fit(UnivariateFinite, v)\n",
       "pdf(d, \"maybe\") # 0.25\n",
       "logpdf(d, \"maybe\") # log(0.25)\n",
       "```\n",
       "\n",
       "One can also do weighted fits:\n",
       "\n",
       "```\n",
       "w = [1, 4, 5, 1] # some weights\n",
       "d = fit(UnivariateFinite, v, w)\n",
       "pdf(d, \"maybe\") ≈ 4/11 # true\n",
       "```\n",
       "\n",
       "See also `classes`, `support`.\n"
      ],
      "text/plain": [
       "\u001b[36m  pdf(d::UnivariateDistribution, x::Real)\u001b[39m\n",
       "\n",
       "  Evaluate the probability density (mass) at \u001b[36mx\u001b[39m.\n",
       "\n",
       "  See also: \u001b[36mlogpdf\u001b[39m.\n",
       "\n",
       "\u001b[36m  pdf(d::MultivariateDistribution, x::AbstractArray)\u001b[39m\n",
       "\n",
       "  Return the probability density of distribution \u001b[36md\u001b[39m evaluated at \u001b[36mx\u001b[39m.\n",
       "\n",
       "    •  If \u001b[36mx\u001b[39m is a vector, it returns the result as a scalar.\n",
       "\n",
       "    •  If \u001b[36mx\u001b[39m is a matrix with n columns, it returns a vector \u001b[36mr\u001b[39m of length n,\n",
       "       where \u001b[36mr[i]\u001b[39m corresponds\n",
       "\n",
       "  to \u001b[36mx[:,i]\u001b[39m (i.e. treating each column as a sample).\n",
       "\n",
       "  \u001b[36mpdf!(r, d, x)\u001b[39m will write the results to a pre-allocated array \u001b[36mr\u001b[39m.\n",
       "\n",
       "\u001b[36m  pdf(d::MatrixDistribution, x::AbstractArray)\u001b[39m\n",
       "\n",
       "  Compute the probability density at the input matrix \u001b[36mx\u001b[39m.\n",
       "\n",
       "\u001b[36m  pdf(d::Union{UnivariateMixture, MultivariateMixture}, x)\u001b[39m\n",
       "\n",
       "  Evaluate the (mixed) probability density function over \u001b[36mx\u001b[39m. Here, \u001b[36mx\u001b[39m can be a\n",
       "  single sample or an array of multiple samples.\n",
       "\n",
       "\u001b[36m  Distributions.pdf(d::UnivariateFinite, x)\u001b[39m\n",
       "\n",
       "  Probability of \u001b[36md\u001b[39m at \u001b[36mx\u001b[39m.\n",
       "\n",
       "\u001b[36m  v = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\u001b[39m\n",
       "\u001b[36m  d = UnivariateFinite(v[1:2], [0.3, 0.7])\u001b[39m\n",
       "\u001b[36m  pdf(d, \"yes\")     # 0.3\u001b[39m\n",
       "\u001b[36m  pdf(d, v[1])      # 0.3\u001b[39m\n",
       "\u001b[36m  pdf(d, \"no\")      # 0.0\u001b[39m\n",
       "\u001b[36m  pdf(d, \"house\")   # throws error\u001b[39m\n",
       "\n",
       "  Other similar methods are available too:\n",
       "\n",
       "\u001b[36m  mode(d)    # CategoricalValue{String, UInt32} \"maybe\"\u001b[39m\n",
       "\u001b[36m  rand(d, 5) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"] or similar\u001b[39m\n",
       "\u001b[36m  d = fit(UnivariateFinite, v)\u001b[39m\n",
       "\u001b[36m  pdf(d, \"maybe\") # 0.25\u001b[39m\n",
       "\u001b[36m  logpdf(d, \"maybe\") # log(0.25)\u001b[39m\n",
       "\n",
       "  One can also do weighted fits:\n",
       "\n",
       "\u001b[36m  w = [1, 4, 5, 1] # some weights\u001b[39m\n",
       "\u001b[36m  d = fit(UnivariateFinite, v, w)\u001b[39m\n",
       "\u001b[36m  pdf(d, \"maybe\") ≈ 4/11 # true\u001b[39m\n",
       "\n",
       "  See also \u001b[36mclasses\u001b[39m, \u001b[36msupport\u001b[39m."
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create pipeline for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
