{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"CSV\", \"DataFrames\", \"Plots\", \"MLJ\", \"MLJFlux\", \"MLJIteration\", \"Statistics\", \n",
    "        \"MLJDecisionTreeInterface\", \"NearestNeighborModels\", \"MLJLinearModels\", \n",
    "        \"MLJMultivariateStatsInterface\", \"MLJLIBSVMInterface\",\"LightGBM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, MLJ, Statistics, Flux, MLJMultivariateStatsInterface, Plots, MLJLinearModels, \n",
    "    MLJLIBSVMInterface, LightGBM\n",
    "import MLJFlux, CSV, MLJDecisionTreeInterface, NearestNeighborModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.File(\"data/train.csv\") |> DataFrame;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th></tr></thead><tbody><p>12 rows × 7 columns (omitted printing of 2 columns)</p><tr><th>1</th><td>PassengerId</td><td>446.0</td><td>1</td><td>446.0</td><td>891</td></tr><tr><th>2</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td></tr><tr><th>3</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td></tr><tr><th>4</th><td>Name</td><td></td><td>Abbing, Mr. Anthony</td><td></td><td>van Melkebeke, Mr. Philemon</td></tr><tr><th>5</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td></tr><tr><th>6</th><td>Age</td><td>29.6991</td><td>0.42</td><td>28.0</td><td>80.0</td></tr><tr><th>7</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td></tr><tr><th>8</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td></tr><tr><th>9</th><td>Ticket</td><td></td><td>110152</td><td></td><td>WE/P 5735</td></tr><tr><th>10</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td></tr><tr><th>11</th><td>Cabin</td><td></td><td>A10</td><td></td><td>T</td></tr><tr><th>12</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& variable & mean & min & median & max & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & \\\\\n",
       "\t\\hline\n",
       "\t1 & PassengerId & 446.0 & 1 & 446.0 & 891 & $\\dots$ \\\\\n",
       "\t2 & Survived & 0.383838 & 0 & 0.0 & 1 & $\\dots$ \\\\\n",
       "\t3 & Pclass & 2.30864 & 1 & 3.0 & 3 & $\\dots$ \\\\\n",
       "\t4 & Name &  & Abbing, Mr. Anthony &  & van Melkebeke, Mr. Philemon & $\\dots$ \\\\\n",
       "\t5 & Sex &  & female &  & male & $\\dots$ \\\\\n",
       "\t6 & Age & 29.6991 & 0.42 & 28.0 & 80.0 & $\\dots$ \\\\\n",
       "\t7 & SibSp & 0.523008 & 0 & 0.0 & 8 & $\\dots$ \\\\\n",
       "\t8 & Parch & 0.381594 & 0 & 0.0 & 6 & $\\dots$ \\\\\n",
       "\t9 & Ticket &  & 110152 &  & WE/P 5735 & $\\dots$ \\\\\n",
       "\t10 & Fare & 32.2042 & 0.0 & 14.4542 & 512.329 & $\\dots$ \\\\\n",
       "\t11 & Cabin &  & A10 &  & T & $\\dots$ \\\\\n",
       "\t12 & Embarked &  & C &  & S & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m12×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable    \u001b[0m\u001b[1m mean     \u001b[0m\u001b[1m min                 \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max               \u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol      \u001b[0m\u001b[90m Union…   \u001b[0m\u001b[90m Any                 \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any               \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ PassengerId  446.0     1                    446.0    891                ⋯\n",
       "   2 │ Survived     0.383838  0                    0.0      1\n",
       "   3 │ Pclass       2.30864   1                    3.0      3\n",
       "   4 │ Name        \u001b[90m          \u001b[0m Abbing, Mr. Anthony \u001b[90m         \u001b[0m van Melkebeke, Mr.\n",
       "   5 │ Sex         \u001b[90m          \u001b[0m female              \u001b[90m         \u001b[0m male               ⋯\n",
       "   6 │ Age          29.6991   0.42                 28.0     80.0\n",
       "   7 │ SibSp        0.523008  0                    0.0      8\n",
       "   8 │ Parch        0.381594  0                    0.0      6\n",
       "   9 │ Ticket      \u001b[90m          \u001b[0m 110152              \u001b[90m         \u001b[0m WE/P 5735          ⋯\n",
       "  10 │ Fare         32.2042   0.0                  14.4542  512.329\n",
       "  11 │ Cabin       \u001b[90m          \u001b[0m A10                 \u001b[90m         \u001b[0m T\n",
       "  12 │ Embarked    \u001b[90m          \u001b[0m C                   \u001b[90m         \u001b[0m S\n",
       "\u001b[36m                                                               3 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sex column\n",
    "male -> 0, female -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element PooledArrays.PooledVector{Int64, UInt32, Vector{UInt32}}:\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sex = map(x -> x == \"male\" ? 0 : 1, data.Sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract title from name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "substring_in_string (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all titles found in name\n",
    "titles_list = [\"Mrs\", \"Mr\", \"Master\", \"Miss\", \"Major\", \"Rev\",\n",
    "                    \"Dr\", \"Ms\", \"Mlle\",\"Col\", \"Capt\", \"Mme\", \"Countess\",\n",
    "                    \"Don\", \"Jonkheer\"]\n",
    "\n",
    "# function to translate name to title eg: mr, miss, master, doctor etc\n",
    "function substring_in_string(big_string::String, substrings::Vector{String})\n",
    "    for substring in substrings\n",
    "        if occursin(substring, big_string)\n",
    "            return substring\n",
    "        end\n",
    "    end\n",
    "    return missing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{String}:\n",
       " \"Mr\"\n",
       " \"Mrs\"\n",
       " \"Miss\"\n",
       " \"Mrs\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Master\"\n",
       " \"Mrs\"\n",
       " \"Mrs\"\n",
       " \"Miss\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " ⋮\n",
       " \"Mrs\"\n",
       " \"Mrs\"\n",
       " \"Mr\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Mrs\"\n",
       " \"Rev\"\n",
       " \"Miss\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " \"Mr\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename!(data, :Name => :Title)\n",
    "data.Title = map(x -> substring_in_string(x, titles_list), data.Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot how frequent each title is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{String}:\n",
       " \"Mr\"\n",
       " \"Mrs\"\n",
       " \"Miss\"\n",
       " \"Mrs\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Master\"\n",
       " \"Mrs\"\n",
       " \"Mrs\"\n",
       " \"Miss\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " ⋮\n",
       " \"Mrs\"\n",
       " \"Mrs\"\n",
       " \"Mr\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " \"Mr\"\n",
       " \"Mrs\"\n",
       " \"Rev\"\n",
       " \"Miss\"\n",
       " \"Miss\"\n",
       " \"Mr\"\n",
       " \"Mr\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_titles = data.Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{String}:\n",
       " \"Mr\"\n",
       " \"Mrs\"\n",
       " \"Miss\"\n",
       " \"Master\"\n",
       " \"Don\"\n",
       " \"Rev\"\n",
       " \"Dr\"\n",
       " \"Mme\"\n",
       " \"Ms\"\n",
       " \"Major\"\n",
       " \"Mlle\"\n",
       " \"Col\"\n",
       " \"Capt\"\n",
       " \"Countess\"\n",
       " \"Jonkheer\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = unique(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_array = []\n",
    "for title in titles\n",
    "    freq_of_title = count(all_titles .== title)\n",
    "    push!(freq_array, freq_of_title)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip170\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip170)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip171\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip170)\" d=\"\n",
       "M235.283 1423.18 L2352.76 1423.18 L2352.76 123.472 L235.283 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip172\">\n",
       "    <rect x=\"235\" y=\"123\" width=\"2118\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  402.681,1423.18 402.681,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  530.015,1423.18 530.015,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  657.349,1423.18 657.349,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  784.683,1423.18 784.683,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  912.017,1423.18 912.017,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1039.35,1423.18 1039.35,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1166.69,1423.18 1166.69,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1294.02,1423.18 1294.02,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1421.35,1423.18 1421.35,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1548.69,1423.18 1548.69,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1676.02,1423.18 1676.02,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1803.36,1423.18 1803.36,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1930.69,1423.18 1930.69,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2058.02,1423.18 2058.02,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2185.36,1423.18 2185.36,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  402.681,1423.18 402.681,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  530.015,1423.18 530.015,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  657.349,1423.18 657.349,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  784.683,1423.18 784.683,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  912.017,1423.18 912.017,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1039.35,1423.18 1039.35,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1166.69,1423.18 1166.69,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1294.02,1423.18 1294.02,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1421.35,1423.18 1421.35,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1548.69,1423.18 1548.69,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1676.02,1423.18 1676.02,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1803.36,1423.18 1803.36,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1930.69,1423.18 1930.69,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2058.02,1423.18 2058.02,1407.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2185.36,1423.18 2185.36,1407.58 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip170)\" d=\"M 0 0 M374.811 1449.29 L381.779 1449.29 L390.598 1472.81 L399.464 1449.29 L406.431 1449.29 L406.431 1483.85 L401.871 1483.85 L401.871 1453.5 L392.959 1477.21 L388.26 1477.21 L379.348 1453.5 L379.348 1483.85 L374.811 1483.85 L374.811 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M430.551 1461.91 Q429.834 1461.49 428.977 1461.31 Q428.144 1461.1 427.126 1461.1 Q423.514 1461.1 421.57 1463.46 Q419.649 1465.8 419.649 1470.19 L419.649 1483.85 L415.366 1483.85 L415.366 1457.93 L419.649 1457.93 L419.649 1461.95 Q420.991 1459.59 423.144 1458.46 Q425.297 1457.3 428.376 1457.3 Q428.815 1457.3 429.348 1457.37 Q429.88 1457.42 430.528 1457.53 L430.551 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M490.953 1449.29 L497.921 1449.29 L506.74 1472.81 L515.606 1449.29 L522.573 1449.29 L522.573 1483.85 L518.013 1483.85 L518.013 1453.5 L509.101 1477.21 L504.402 1477.21 L495.49 1453.5 L495.49 1483.85 L490.953 1483.85 L490.953 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M546.693 1461.91 Q545.976 1461.49 545.119 1461.31 Q544.286 1461.1 543.268 1461.1 Q539.656 1461.1 537.712 1463.46 Q535.791 1465.8 535.791 1470.19 L535.791 1483.85 L531.508 1483.85 L531.508 1457.93 L535.791 1457.93 L535.791 1461.95 Q537.133 1459.59 539.286 1458.46 Q541.439 1457.3 544.518 1457.3 Q544.957 1457.3 545.49 1457.37 Q546.022 1457.42 546.67 1457.53 L546.693 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M567.689 1458.69 L567.689 1462.72 Q565.883 1461.79 563.939 1461.33 Q561.994 1460.87 559.911 1460.87 Q556.74 1460.87 555.142 1461.84 Q553.568 1462.81 553.568 1464.75 Q553.568 1466.24 554.703 1467.09 Q555.837 1467.93 559.263 1468.69 L560.721 1469.01 Q565.258 1469.99 567.156 1471.77 Q569.078 1473.53 569.078 1476.7 Q569.078 1480.31 566.207 1482.42 Q563.36 1484.52 558.36 1484.52 Q556.277 1484.52 554.008 1484.11 Q551.763 1483.71 549.263 1482.9 L549.263 1478.5 Q551.624 1479.73 553.916 1480.36 Q556.207 1480.96 558.453 1480.96 Q561.462 1480.96 563.082 1479.94 Q564.703 1478.9 564.703 1477.02 Q564.703 1475.29 563.522 1474.36 Q562.365 1473.44 558.406 1472.58 L556.925 1472.23 Q552.967 1471.4 551.207 1469.69 Q549.448 1467.95 549.448 1464.94 Q549.448 1461.28 552.041 1459.29 Q554.633 1457.3 559.402 1457.3 Q561.763 1457.3 563.846 1457.65 Q565.929 1458 567.689 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M609.097 1449.29 L616.065 1449.29 L624.884 1472.81 L633.75 1449.29 L640.717 1449.29 L640.717 1483.85 L636.157 1483.85 L636.157 1453.5 L627.245 1477.21 L622.546 1477.21 L613.634 1453.5 L613.634 1483.85 L609.097 1483.85 L609.097 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M649.815 1457.93 L654.074 1457.93 L654.074 1483.85 L649.815 1483.85 L649.815 1457.93 M649.815 1447.83 L654.074 1447.83 L654.074 1453.23 L649.815 1453.23 L649.815 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M679.514 1458.69 L679.514 1462.72 Q677.708 1461.79 675.764 1461.33 Q673.819 1460.87 671.736 1460.87 Q668.565 1460.87 666.967 1461.84 Q665.393 1462.81 665.393 1464.75 Q665.393 1466.24 666.527 1467.09 Q667.662 1467.93 671.088 1468.69 L672.546 1469.01 Q677.083 1469.99 678.981 1471.77 Q680.902 1473.53 680.902 1476.7 Q680.902 1480.31 678.032 1482.42 Q675.185 1484.52 670.185 1484.52 Q668.102 1484.52 665.833 1484.11 Q663.588 1483.71 661.088 1482.9 L661.088 1478.5 Q663.449 1479.73 665.74 1480.36 Q668.032 1480.96 670.277 1480.96 Q673.287 1480.96 674.907 1479.94 Q676.527 1478.9 676.527 1477.02 Q676.527 1475.29 675.347 1474.36 Q674.189 1473.44 670.231 1472.58 L668.75 1472.23 Q664.791 1471.4 663.032 1469.69 Q661.273 1467.95 661.273 1464.94 Q661.273 1461.28 663.865 1459.29 Q666.458 1457.3 671.227 1457.3 Q673.588 1457.3 675.671 1457.65 Q677.754 1458 679.514 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M704.212 1458.69 L704.212 1462.72 Q702.407 1461.79 700.462 1461.33 Q698.518 1460.87 696.435 1460.87 Q693.263 1460.87 691.666 1461.84 Q690.092 1462.81 690.092 1464.75 Q690.092 1466.24 691.226 1467.09 Q692.361 1467.93 695.787 1468.69 L697.245 1469.01 Q701.782 1469.99 703.68 1471.77 Q705.601 1473.53 705.601 1476.7 Q705.601 1480.31 702.731 1482.42 Q699.884 1484.52 694.884 1484.52 Q692.8 1484.52 690.532 1484.11 Q688.287 1483.71 685.787 1482.9 L685.787 1478.5 Q688.148 1479.73 690.439 1480.36 Q692.731 1480.96 694.976 1480.96 Q697.986 1480.96 699.606 1479.94 Q701.226 1478.9 701.226 1477.02 Q701.226 1475.29 700.046 1474.36 Q698.888 1473.44 694.93 1472.58 L693.449 1472.23 Q689.49 1471.4 687.731 1469.69 Q685.972 1467.95 685.972 1464.94 Q685.972 1461.28 688.564 1459.29 Q691.157 1457.3 695.925 1457.3 Q698.287 1457.3 700.37 1457.65 Q702.453 1458 704.212 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M706.061 1449.29 L713.029 1449.29 L721.848 1472.81 L730.714 1449.29 L737.681 1449.29 L737.681 1483.85 L733.121 1483.85 L733.121 1453.5 L724.209 1477.21 L719.51 1477.21 L710.598 1453.5 L710.598 1483.85 L706.061 1483.85 L706.061 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M758.561 1470.82 Q753.399 1470.82 751.408 1472 Q749.417 1473.18 749.417 1476.03 Q749.417 1478.3 750.899 1479.64 Q752.403 1480.96 754.973 1480.96 Q758.514 1480.96 760.644 1478.46 Q762.797 1475.94 762.797 1471.77 L762.797 1470.82 L758.561 1470.82 M767.056 1469.06 L767.056 1483.85 L762.797 1483.85 L762.797 1479.92 Q761.339 1482.28 759.163 1483.41 Q756.987 1484.52 753.839 1484.52 Q749.857 1484.52 747.496 1482.3 Q745.158 1480.06 745.158 1476.31 Q745.158 1471.93 748.075 1469.71 Q751.015 1467.49 756.825 1467.49 L762.797 1467.49 L762.797 1467.07 Q762.797 1464.13 760.852 1462.53 Q758.931 1460.91 755.436 1460.91 Q753.214 1460.91 751.107 1461.44 Q749.001 1461.98 747.056 1463.04 L747.056 1459.11 Q749.394 1458.2 751.593 1457.76 Q753.792 1457.3 755.876 1457.3 Q761.501 1457.3 764.278 1460.22 Q767.056 1463.13 767.056 1469.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M792.357 1458.69 L792.357 1462.72 Q790.551 1461.79 788.607 1461.33 Q786.662 1460.87 784.579 1460.87 Q781.408 1460.87 779.811 1461.84 Q778.237 1462.81 778.237 1464.75 Q778.237 1466.24 779.371 1467.09 Q780.505 1467.93 783.931 1468.69 L785.389 1469.01 Q789.926 1469.99 791.824 1471.77 Q793.746 1473.53 793.746 1476.7 Q793.746 1480.31 790.875 1482.42 Q788.028 1484.52 783.028 1484.52 Q780.945 1484.52 778.676 1484.11 Q776.431 1483.71 773.931 1482.9 L773.931 1478.5 Q776.292 1479.73 778.584 1480.36 Q780.875 1480.96 783.121 1480.96 Q786.13 1480.96 787.75 1479.94 Q789.371 1478.9 789.371 1477.02 Q789.371 1475.29 788.19 1474.36 Q787.033 1473.44 783.075 1472.58 L781.593 1472.23 Q777.635 1471.4 775.875 1469.69 Q774.116 1467.95 774.116 1464.94 Q774.116 1461.28 776.709 1459.29 Q779.301 1457.3 784.07 1457.3 Q786.431 1457.3 788.514 1457.65 Q790.598 1458 792.357 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M804.741 1450.56 L804.741 1457.93 L813.514 1457.93 L813.514 1461.24 L804.741 1461.24 L804.741 1475.31 Q804.741 1478.48 805.598 1479.38 Q806.477 1480.29 809.139 1480.29 L813.514 1480.29 L813.514 1483.85 L809.139 1483.85 Q804.209 1483.85 802.334 1482.02 Q800.459 1480.17 800.459 1475.31 L800.459 1461.24 L797.334 1461.24 L797.334 1457.93 L800.459 1457.93 L800.459 1450.56 L804.741 1450.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M841.292 1469.82 L841.292 1471.91 L821.709 1471.91 Q821.986 1476.31 824.347 1478.62 Q826.732 1480.91 830.968 1480.91 Q833.421 1480.91 835.713 1480.31 Q838.028 1479.71 840.296 1478.5 L840.296 1482.53 Q838.005 1483.5 835.597 1484.01 Q833.19 1484.52 830.713 1484.52 Q824.509 1484.52 820.875 1480.91 Q817.264 1477.3 817.264 1471.14 Q817.264 1464.78 820.69 1461.05 Q824.139 1457.3 829.972 1457.3 Q835.204 1457.3 838.236 1460.68 Q841.292 1464.04 841.292 1469.82 M837.033 1468.57 Q836.986 1465.08 835.065 1463 Q833.167 1460.91 830.019 1460.91 Q826.454 1460.91 824.301 1462.93 Q822.172 1464.94 821.847 1468.6 L837.033 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M863.306 1461.91 Q862.588 1461.49 861.732 1461.31 Q860.898 1461.1 859.88 1461.1 Q856.269 1461.1 854.324 1463.46 Q852.403 1465.8 852.403 1470.19 L852.403 1483.85 L848.12 1483.85 L848.12 1457.93 L852.403 1457.93 L852.403 1461.95 Q853.745 1459.59 855.898 1458.46 Q858.051 1457.3 861.13 1457.3 Q861.569 1457.3 862.102 1457.37 Q862.634 1457.42 863.282 1457.53 L863.306 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M873.256 1453.13 L873.256 1480.01 L878.904 1480.01 Q886.057 1480.01 889.367 1476.77 Q892.7 1473.53 892.7 1466.54 Q892.7 1459.59 889.367 1456.38 Q886.057 1453.13 878.904 1453.13 L873.256 1453.13 M868.58 1449.29 L878.186 1449.29 Q888.233 1449.29 892.932 1453.48 Q897.631 1457.65 897.631 1466.54 Q897.631 1475.47 892.909 1479.66 Q888.186 1483.85 878.186 1483.85 L868.58 1483.85 L868.58 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M914.946 1460.91 Q911.52 1460.91 909.529 1463.6 Q907.538 1466.26 907.538 1470.91 Q907.538 1475.56 909.506 1478.25 Q911.496 1480.91 914.946 1480.91 Q918.348 1480.91 920.339 1478.23 Q922.33 1475.54 922.33 1470.91 Q922.33 1466.31 920.339 1463.62 Q918.348 1460.91 914.946 1460.91 M914.946 1457.3 Q920.501 1457.3 923.672 1460.91 Q926.844 1464.52 926.844 1470.91 Q926.844 1477.28 923.672 1480.91 Q920.501 1484.52 914.946 1484.52 Q909.367 1484.52 906.196 1480.91 Q903.047 1477.28 903.047 1470.91 Q903.047 1464.52 906.196 1460.91 Q909.367 1457.3 914.946 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M955.455 1468.2 L955.455 1483.85 L951.195 1483.85 L951.195 1468.34 Q951.195 1464.66 949.76 1462.83 Q948.325 1461 945.455 1461 Q942.006 1461 940.015 1463.2 Q938.024 1465.4 938.024 1469.2 L938.024 1483.85 L933.742 1483.85 L933.742 1457.93 L938.024 1457.93 L938.024 1461.95 Q939.552 1459.62 941.612 1458.46 Q943.695 1457.3 946.404 1457.3 Q950.871 1457.3 953.163 1460.08 Q955.455 1462.83 955.455 1468.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1014.76 1467.65 Q1016.26 1468.16 1017.67 1469.82 Q1019.11 1471.49 1020.54 1474.41 L1025.29 1483.85 L1020.27 1483.85 L1015.84 1474.99 Q1014.13 1471.51 1012.51 1470.38 Q1010.91 1469.25 1008.14 1469.25 L1003.04 1469.25 L1003.04 1483.85 L998.368 1483.85 L998.368 1449.29 L1008.92 1449.29 Q1014.85 1449.29 1017.77 1451.77 Q1020.68 1454.25 1020.68 1459.25 Q1020.68 1462.51 1019.15 1464.66 Q1017.65 1466.81 1014.76 1467.65 M1003.04 1453.13 L1003.04 1465.4 L1008.92 1465.4 Q1012.3 1465.4 1014.02 1463.85 Q1015.75 1462.28 1015.75 1459.25 Q1015.75 1456.21 1014.02 1454.69 Q1012.3 1453.13 1008.92 1453.13 L1003.04 1453.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1051.17 1469.82 L1051.17 1471.91 L1031.59 1471.91 Q1031.86 1476.31 1034.22 1478.62 Q1036.61 1480.91 1040.84 1480.91 Q1043.3 1480.91 1045.59 1480.31 Q1047.9 1479.71 1050.17 1478.5 L1050.17 1482.53 Q1047.88 1483.5 1045.47 1484.01 Q1043.07 1484.52 1040.59 1484.52 Q1034.39 1484.52 1030.75 1480.91 Q1027.14 1477.3 1027.14 1471.14 Q1027.14 1464.78 1030.57 1461.05 Q1034.02 1457.3 1039.85 1457.3 Q1045.08 1457.3 1048.11 1460.68 Q1051.17 1464.04 1051.17 1469.82 M1046.91 1468.57 Q1046.86 1465.08 1044.94 1463 Q1043.04 1460.91 1039.9 1460.91 Q1036.33 1460.91 1034.18 1462.93 Q1032.05 1464.94 1031.72 1468.6 L1046.91 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1055.1 1457.93 L1059.62 1457.93 L1067.72 1479.69 L1075.82 1457.93 L1080.33 1457.93 L1070.61 1483.85 L1064.83 1483.85 L1055.1 1457.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1145.69 1453.13 L1145.69 1480.01 L1151.34 1480.01 Q1158.49 1480.01 1161.8 1476.77 Q1165.13 1473.53 1165.13 1466.54 Q1165.13 1459.59 1161.8 1456.38 Q1158.49 1453.13 1151.34 1453.13 L1145.69 1453.13 M1141.01 1449.29 L1150.62 1449.29 Q1160.67 1449.29 1165.37 1453.48 Q1170.06 1457.65 1170.06 1466.54 Q1170.06 1475.47 1165.34 1479.66 Q1160.62 1483.85 1150.62 1483.85 L1141.01 1483.85 L1141.01 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1192.36 1461.91 Q1191.64 1461.49 1190.78 1461.31 Q1189.95 1461.1 1188.93 1461.1 Q1185.32 1461.1 1183.38 1463.46 Q1181.45 1465.8 1181.45 1470.19 L1181.45 1483.85 L1177.17 1483.85 L1177.17 1457.93 L1181.45 1457.93 L1181.45 1461.95 Q1182.8 1459.59 1184.95 1458.46 Q1187.1 1457.3 1190.18 1457.3 Q1190.62 1457.3 1191.15 1457.37 Q1191.69 1457.42 1192.33 1457.53 L1192.36 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1239.48 1449.29 L1246.45 1449.29 L1255.27 1472.81 L1264.14 1449.29 L1271.1 1449.29 L1271.1 1483.85 L1266.54 1483.85 L1266.54 1453.5 L1257.63 1477.21 L1252.93 1477.21 L1244.02 1453.5 L1244.02 1483.85 L1239.48 1483.85 L1239.48 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1300.39 1462.9 Q1301.98 1460.03 1304.2 1458.67 Q1306.43 1457.3 1309.44 1457.3 Q1313.49 1457.3 1315.69 1460.15 Q1317.88 1462.97 1317.88 1468.2 L1317.88 1483.85 L1313.6 1483.85 L1313.6 1468.34 Q1313.6 1464.62 1312.28 1462.81 Q1310.96 1461 1308.26 1461 Q1304.95 1461 1303.02 1463.2 Q1301.1 1465.4 1301.1 1469.2 L1301.1 1483.85 L1296.82 1483.85 L1296.82 1468.34 Q1296.82 1464.59 1295.5 1462.81 Q1294.18 1461 1291.43 1461 Q1288.16 1461 1286.24 1463.23 Q1284.32 1465.43 1284.32 1469.2 L1284.32 1483.85 L1280.04 1483.85 L1280.04 1457.93 L1284.32 1457.93 L1284.32 1461.95 Q1285.78 1459.57 1287.82 1458.44 Q1289.85 1457.3 1292.65 1457.3 Q1295.48 1457.3 1297.45 1458.74 Q1299.44 1460.17 1300.39 1462.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1348.56 1469.82 L1348.56 1471.91 L1328.97 1471.91 Q1329.25 1476.31 1331.61 1478.62 Q1334 1480.91 1338.23 1480.91 Q1340.69 1480.91 1342.98 1480.31 Q1345.29 1479.71 1347.56 1478.5 L1347.56 1482.53 Q1345.27 1483.5 1342.86 1484.01 Q1340.45 1484.52 1337.98 1484.52 Q1331.77 1484.52 1328.14 1480.91 Q1324.53 1477.3 1324.53 1471.14 Q1324.53 1464.78 1327.95 1461.05 Q1331.4 1457.3 1337.24 1457.3 Q1342.47 1457.3 1345.5 1460.68 Q1348.56 1464.04 1348.56 1469.82 M1344.3 1468.57 Q1344.25 1465.08 1342.33 1463 Q1340.43 1460.91 1337.28 1460.91 Q1333.72 1460.91 1331.57 1462.93 Q1329.44 1464.94 1329.11 1468.6 L1344.3 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1392.04 1449.29 L1399 1449.29 L1407.82 1472.81 L1416.69 1449.29 L1423.66 1449.29 L1423.66 1483.85 L1419.1 1483.85 L1419.1 1453.5 L1410.18 1477.21 L1405.49 1477.21 L1396.57 1453.5 L1396.57 1483.85 L1392.04 1483.85 L1392.04 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1449.28 1458.69 L1449.28 1462.72 Q1447.48 1461.79 1445.53 1461.33 Q1443.59 1460.87 1441.5 1460.87 Q1438.33 1460.87 1436.74 1461.84 Q1435.16 1462.81 1435.16 1464.75 Q1435.16 1466.24 1436.3 1467.09 Q1437.43 1467.93 1440.86 1468.69 L1442.31 1469.01 Q1446.85 1469.99 1448.75 1471.77 Q1450.67 1473.53 1450.67 1476.7 Q1450.67 1480.31 1447.8 1482.42 Q1444.95 1484.52 1439.95 1484.52 Q1437.87 1484.52 1435.6 1484.11 Q1433.36 1483.71 1430.86 1482.9 L1430.86 1478.5 Q1433.22 1479.73 1435.51 1480.36 Q1437.8 1480.96 1440.05 1480.96 Q1443.05 1480.96 1444.68 1479.94 Q1446.3 1478.9 1446.3 1477.02 Q1446.3 1475.29 1445.11 1474.36 Q1443.96 1473.44 1440 1472.58 L1438.52 1472.23 Q1434.56 1471.4 1432.8 1469.69 Q1431.04 1467.95 1431.04 1464.94 Q1431.04 1461.28 1433.63 1459.29 Q1436.23 1457.3 1440.99 1457.3 Q1443.36 1457.3 1445.44 1457.65 Q1447.52 1458 1449.28 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1485.2 1449.29 L1492.17 1449.29 L1500.99 1472.81 L1509.86 1449.29 L1516.82 1449.29 L1516.82 1483.85 L1512.26 1483.85 L1512.26 1453.5 L1503.35 1477.21 L1498.65 1477.21 L1489.74 1453.5 L1489.74 1483.85 L1485.2 1483.85 L1485.2 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1537.7 1470.82 Q1532.54 1470.82 1530.55 1472 Q1528.56 1473.18 1528.56 1476.03 Q1528.56 1478.3 1530.04 1479.64 Q1531.55 1480.96 1534.12 1480.96 Q1537.66 1480.96 1539.79 1478.46 Q1541.94 1475.94 1541.94 1471.77 L1541.94 1470.82 L1537.7 1470.82 M1546.2 1469.06 L1546.2 1483.85 L1541.94 1483.85 L1541.94 1479.92 Q1540.48 1482.28 1538.31 1483.41 Q1536.13 1484.52 1532.98 1484.52 Q1529 1484.52 1526.64 1482.3 Q1524.3 1480.06 1524.3 1476.31 Q1524.3 1471.93 1527.22 1469.71 Q1530.16 1467.49 1535.97 1467.49 L1541.94 1467.49 L1541.94 1467.07 Q1541.94 1464.13 1540 1462.53 Q1538.07 1460.91 1534.58 1460.91 Q1532.36 1460.91 1530.25 1461.44 Q1528.14 1461.98 1526.2 1463.04 L1526.2 1459.11 Q1528.54 1458.2 1530.74 1457.76 Q1532.94 1457.3 1535.02 1457.3 Q1540.64 1457.3 1543.42 1460.22 Q1546.2 1463.13 1546.2 1469.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1554.97 1457.93 L1559.23 1457.93 L1559.23 1484.31 Q1559.23 1489.27 1557.33 1491.49 Q1555.46 1493.71 1551.27 1493.71 L1549.65 1493.71 L1549.65 1490.1 L1550.78 1490.1 Q1553.21 1490.1 1554.09 1488.97 Q1554.97 1487.86 1554.97 1484.31 L1554.97 1457.93 M1554.97 1447.83 L1559.23 1447.83 L1559.23 1453.23 L1554.97 1453.23 L1554.97 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1578.19 1460.91 Q1574.76 1460.91 1572.77 1463.6 Q1570.78 1466.26 1570.78 1470.91 Q1570.78 1475.56 1572.75 1478.25 Q1574.74 1480.91 1578.19 1480.91 Q1581.59 1480.91 1583.58 1478.23 Q1585.57 1475.54 1585.57 1470.91 Q1585.57 1466.31 1583.58 1463.62 Q1581.59 1460.91 1578.19 1460.91 M1578.19 1457.3 Q1583.75 1457.3 1586.92 1460.91 Q1590.09 1464.52 1590.09 1470.91 Q1590.09 1477.28 1586.92 1480.91 Q1583.75 1484.52 1578.19 1484.52 Q1572.61 1484.52 1569.44 1480.91 Q1566.29 1477.28 1566.29 1470.91 Q1566.29 1464.52 1569.44 1460.91 Q1572.61 1457.3 1578.19 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1612.17 1461.91 Q1611.45 1461.49 1610.6 1461.31 Q1609.76 1461.1 1608.74 1461.1 Q1605.13 1461.1 1603.19 1463.46 Q1601.27 1465.8 1601.27 1470.19 L1601.27 1483.85 L1596.99 1483.85 L1596.99 1457.93 L1601.27 1457.93 L1601.27 1461.95 Q1602.61 1459.59 1604.76 1458.46 Q1606.92 1457.3 1609.99 1457.3 Q1610.43 1457.3 1610.97 1457.37 Q1611.5 1457.42 1612.15 1457.53 L1612.17 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1631.4 1449.29 L1638.37 1449.29 L1647.19 1472.81 L1656.06 1449.29 L1663.02 1449.29 L1663.02 1483.85 L1658.46 1483.85 L1658.46 1453.5 L1649.55 1477.21 L1644.85 1477.21 L1635.94 1453.5 L1635.94 1483.85 L1631.4 1483.85 L1631.4 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1672.12 1447.83 L1676.38 1447.83 L1676.38 1483.85 L1672.12 1483.85 L1672.12 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1685.29 1447.83 L1689.55 1447.83 L1689.55 1483.85 L1685.29 1483.85 L1685.29 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1720.64 1469.82 L1720.64 1471.91 L1701.06 1471.91 Q1701.33 1476.31 1703.69 1478.62 Q1706.08 1480.91 1710.32 1480.91 Q1712.77 1480.91 1715.06 1480.31 Q1717.38 1479.71 1719.64 1478.5 L1719.64 1482.53 Q1717.35 1483.5 1714.94 1484.01 Q1712.54 1484.52 1710.06 1484.52 Q1703.86 1484.52 1700.22 1480.91 Q1696.61 1477.3 1696.61 1471.14 Q1696.61 1464.78 1700.04 1461.05 Q1703.49 1457.3 1709.32 1457.3 Q1714.55 1457.3 1717.58 1460.68 Q1720.64 1464.04 1720.64 1469.82 M1716.38 1468.57 Q1716.33 1465.08 1714.41 1463 Q1712.51 1460.91 1709.37 1460.91 Q1705.8 1460.91 1703.65 1462.93 Q1701.52 1464.94 1701.19 1468.6 L1716.38 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1797.14 1451.95 L1797.14 1456.88 Q1794.78 1454.69 1792.09 1453.6 Q1789.43 1452.51 1786.42 1452.51 Q1780.5 1452.51 1777.35 1456.14 Q1774.2 1459.75 1774.2 1466.61 Q1774.2 1473.44 1777.35 1477.07 Q1780.5 1480.68 1786.42 1480.68 Q1789.43 1480.68 1792.09 1479.59 Q1794.78 1478.5 1797.14 1476.31 L1797.14 1481.19 Q1794.69 1482.86 1791.93 1483.69 Q1789.2 1484.52 1786.14 1484.52 Q1778.3 1484.52 1773.78 1479.73 Q1769.27 1474.92 1769.27 1466.61 Q1769.27 1458.27 1773.78 1453.48 Q1778.3 1448.67 1786.14 1448.67 Q1789.25 1448.67 1791.98 1449.5 Q1794.73 1450.31 1797.14 1451.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1814.22 1460.91 Q1810.8 1460.91 1808.81 1463.6 Q1806.82 1466.26 1806.82 1470.91 Q1806.82 1475.56 1808.78 1478.25 Q1810.77 1480.91 1814.22 1480.91 Q1817.63 1480.91 1819.62 1478.23 Q1821.61 1475.54 1821.61 1470.91 Q1821.61 1466.31 1819.62 1463.62 Q1817.63 1460.91 1814.22 1460.91 M1814.22 1457.3 Q1819.78 1457.3 1822.95 1460.91 Q1826.12 1464.52 1826.12 1470.91 Q1826.12 1477.28 1822.95 1480.91 Q1819.78 1484.52 1814.22 1484.52 Q1808.64 1484.52 1805.47 1480.91 Q1802.33 1477.28 1802.33 1470.91 Q1802.33 1464.52 1805.47 1460.91 Q1808.64 1457.3 1814.22 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1833.18 1447.83 L1837.44 1447.83 L1837.44 1483.85 L1833.18 1483.85 L1833.18 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1905.04 1451.95 L1905.04 1456.88 Q1902.68 1454.69 1900 1453.6 Q1897.33 1452.51 1894.32 1452.51 Q1888.4 1452.51 1885.25 1456.14 Q1882.1 1459.75 1882.1 1466.61 Q1882.1 1473.44 1885.25 1477.07 Q1888.4 1480.68 1894.32 1480.68 Q1897.33 1480.68 1900 1479.59 Q1902.68 1478.5 1905.04 1476.31 L1905.04 1481.19 Q1902.59 1482.86 1899.83 1483.69 Q1897.1 1484.52 1894.05 1484.52 Q1886.2 1484.52 1881.69 1479.73 Q1877.17 1474.92 1877.17 1466.61 Q1877.17 1458.27 1881.69 1453.48 Q1886.2 1448.67 1894.05 1448.67 Q1897.15 1448.67 1899.88 1449.5 Q1902.63 1450.31 1905.04 1451.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1923.86 1470.82 Q1918.7 1470.82 1916.71 1472 Q1914.72 1473.18 1914.72 1476.03 Q1914.72 1478.3 1916.2 1479.64 Q1917.7 1480.96 1920.27 1480.96 Q1923.81 1480.96 1925.94 1478.46 Q1928.1 1475.94 1928.1 1471.77 L1928.1 1470.82 L1923.86 1470.82 M1932.36 1469.06 L1932.36 1483.85 L1928.1 1483.85 L1928.1 1479.92 Q1926.64 1482.28 1924.46 1483.41 Q1922.29 1484.52 1919.14 1484.52 Q1915.16 1484.52 1912.8 1482.3 Q1910.46 1480.06 1910.46 1476.31 Q1910.46 1471.93 1913.37 1469.71 Q1916.31 1467.49 1922.12 1467.49 L1928.1 1467.49 L1928.1 1467.07 Q1928.1 1464.13 1926.15 1462.53 Q1924.23 1460.91 1920.74 1460.91 Q1918.51 1460.91 1916.41 1461.44 Q1914.3 1461.98 1912.36 1463.04 L1912.36 1459.11 Q1914.69 1458.2 1916.89 1457.76 Q1919.09 1457.3 1921.18 1457.3 Q1926.8 1457.3 1929.58 1460.22 Q1932.36 1463.13 1932.36 1469.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1945.25 1479.96 L1945.25 1493.71 L1940.97 1493.71 L1940.97 1457.93 L1945.25 1457.93 L1945.25 1461.86 Q1946.59 1459.55 1948.63 1458.44 Q1950.69 1457.3 1953.54 1457.3 Q1958.26 1457.3 1961.2 1461.05 Q1964.16 1464.8 1964.16 1470.91 Q1964.16 1477.02 1961.2 1480.77 Q1958.26 1484.52 1953.54 1484.52 Q1950.69 1484.52 1948.63 1483.41 Q1946.59 1482.28 1945.25 1479.96 M1959.74 1470.91 Q1959.74 1466.21 1957.8 1463.55 Q1955.87 1460.87 1952.49 1460.87 Q1949.12 1460.87 1947.17 1463.55 Q1945.25 1466.21 1945.25 1470.91 Q1945.25 1475.61 1947.17 1478.3 Q1949.12 1480.96 1952.49 1480.96 Q1955.87 1480.96 1957.8 1478.3 Q1959.74 1475.61 1959.74 1470.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1975.43 1450.56 L1975.43 1457.93 L1984.21 1457.93 L1984.21 1461.24 L1975.43 1461.24 L1975.43 1475.31 Q1975.43 1478.48 1976.29 1479.38 Q1977.17 1480.29 1979.83 1480.29 L1984.21 1480.29 L1984.21 1483.85 L1979.83 1483.85 Q1974.9 1483.85 1973.03 1482.02 Q1971.15 1480.17 1971.15 1475.31 L1971.15 1461.24 L1968.03 1461.24 L1968.03 1457.93 L1971.15 1457.93 L1971.15 1450.56 L1975.43 1450.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1978.71 1451.95 L1978.71 1456.88 Q1976.35 1454.69 1973.66 1453.6 Q1971 1452.51 1967.99 1452.51 Q1962.06 1452.51 1958.92 1456.14 Q1955.77 1459.75 1955.77 1466.61 Q1955.77 1473.44 1958.92 1477.07 Q1962.06 1480.68 1967.99 1480.68 Q1971 1480.68 1973.66 1479.59 Q1976.35 1478.5 1978.71 1476.31 L1978.71 1481.19 Q1976.25 1482.86 1973.5 1483.69 Q1970.77 1484.52 1967.71 1484.52 Q1959.86 1484.52 1955.35 1479.73 Q1950.84 1474.92 1950.84 1466.61 Q1950.84 1458.27 1955.35 1453.48 Q1959.86 1448.67 1967.71 1448.67 Q1970.81 1448.67 1973.54 1449.5 Q1976.3 1450.31 1978.71 1451.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1995.79 1460.91 Q1992.36 1460.91 1990.37 1463.6 Q1988.38 1466.26 1988.38 1470.91 Q1988.38 1475.56 1990.35 1478.25 Q1992.34 1480.91 1995.79 1480.91 Q1999.19 1480.91 2001.18 1478.23 Q2003.17 1475.54 2003.17 1470.91 Q2003.17 1466.31 2001.18 1463.62 Q1999.19 1460.91 1995.79 1460.91 M1995.79 1457.3 Q2001.35 1457.3 2004.52 1460.91 Q2007.69 1464.52 2007.69 1470.91 Q2007.69 1477.28 2004.52 1480.91 Q2001.35 1484.52 1995.79 1484.52 Q1990.21 1484.52 1987.04 1480.91 Q1983.89 1477.28 1983.89 1470.91 Q1983.89 1464.52 1987.04 1460.91 Q1990.21 1457.3 1995.79 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2014.31 1473.62 L2014.31 1457.93 L2018.57 1457.93 L2018.57 1473.46 Q2018.57 1477.14 2020 1478.99 Q2021.44 1480.82 2024.31 1480.82 Q2027.76 1480.82 2029.75 1478.62 Q2031.76 1476.42 2031.76 1472.63 L2031.76 1457.93 L2036.02 1457.93 L2036.02 1483.85 L2031.76 1483.85 L2031.76 1479.87 Q2030.21 1482.23 2028.15 1483.39 Q2026.11 1484.52 2023.41 1484.52 Q2018.94 1484.52 2016.62 1481.75 Q2014.31 1478.97 2014.31 1473.62 M2025.03 1457.3 L2025.03 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2066.35 1468.2 L2066.35 1483.85 L2062.09 1483.85 L2062.09 1468.34 Q2062.09 1464.66 2060.65 1462.83 Q2059.22 1461 2056.35 1461 Q2052.9 1461 2050.91 1463.2 Q2048.91 1465.4 2048.91 1469.2 L2048.91 1483.85 L2044.63 1483.85 L2044.63 1457.93 L2048.91 1457.93 L2048.91 1461.95 Q2050.44 1459.62 2052.5 1458.46 Q2054.59 1457.3 2057.29 1457.3 Q2061.76 1457.3 2064.05 1460.08 Q2066.35 1462.83 2066.35 1468.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2079.05 1450.56 L2079.05 1457.93 L2087.83 1457.93 L2087.83 1461.24 L2079.05 1461.24 L2079.05 1475.31 Q2079.05 1478.48 2079.91 1479.38 Q2080.79 1480.29 2083.45 1480.29 L2087.83 1480.29 L2087.83 1483.85 L2083.45 1483.85 Q2078.52 1483.85 2076.65 1482.02 Q2074.77 1480.17 2074.77 1475.31 L2074.77 1461.24 L2071.65 1461.24 L2071.65 1457.93 L2074.77 1457.93 L2074.77 1450.56 L2079.05 1450.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2115.6 1469.82 L2115.6 1471.91 L2096.02 1471.91 Q2096.3 1476.31 2098.66 1478.62 Q2101.04 1480.91 2105.28 1480.91 Q2107.73 1480.91 2110.03 1480.31 Q2112.34 1479.71 2114.61 1478.5 L2114.61 1482.53 Q2112.32 1483.5 2109.91 1484.01 Q2107.5 1484.52 2105.03 1484.52 Q2098.82 1484.52 2095.19 1480.91 Q2091.58 1477.3 2091.58 1471.14 Q2091.58 1464.78 2095 1461.05 Q2098.45 1457.3 2104.28 1457.3 Q2109.52 1457.3 2112.55 1460.68 Q2115.6 1464.04 2115.6 1469.82 M2111.34 1468.57 Q2111.3 1465.08 2109.38 1463 Q2107.48 1460.91 2104.33 1460.91 Q2100.77 1460.91 2098.61 1462.93 Q2096.48 1464.94 2096.16 1468.6 L2111.34 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2139.12 1458.69 L2139.12 1462.72 Q2137.32 1461.79 2135.37 1461.33 Q2133.43 1460.87 2131.34 1460.87 Q2128.17 1460.87 2126.58 1461.84 Q2125 1462.81 2125 1464.75 Q2125 1466.24 2126.14 1467.09 Q2127.27 1467.93 2130.7 1468.69 L2132.16 1469.01 Q2136.69 1469.99 2138.59 1471.77 Q2140.51 1473.53 2140.51 1476.7 Q2140.51 1480.31 2137.64 1482.42 Q2134.79 1484.52 2129.79 1484.52 Q2127.71 1484.52 2125.44 1484.11 Q2123.2 1483.71 2120.7 1482.9 L2120.7 1478.5 Q2123.06 1479.73 2125.35 1480.36 Q2127.64 1480.96 2129.89 1480.96 Q2132.9 1480.96 2134.52 1479.94 Q2136.14 1478.9 2136.14 1477.02 Q2136.14 1475.29 2134.96 1474.36 Q2133.8 1473.44 2129.84 1472.58 L2128.36 1472.23 Q2124.4 1471.4 2122.64 1469.69 Q2120.88 1467.95 2120.88 1464.94 Q2120.88 1461.28 2123.47 1459.29 Q2126.07 1457.3 2130.84 1457.3 Q2133.2 1457.3 2135.28 1457.65 Q2137.36 1458 2139.12 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2163.82 1458.69 L2163.82 1462.72 Q2162.02 1461.79 2160.07 1461.33 Q2158.13 1460.87 2156.04 1460.87 Q2152.87 1460.87 2151.28 1461.84 Q2149.7 1462.81 2149.7 1464.75 Q2149.7 1466.24 2150.84 1467.09 Q2151.97 1467.93 2155.4 1468.69 L2156.85 1469.01 Q2161.39 1469.99 2163.29 1471.77 Q2165.21 1473.53 2165.21 1476.7 Q2165.21 1480.31 2162.34 1482.42 Q2159.49 1484.52 2154.49 1484.52 Q2152.41 1484.52 2150.14 1484.11 Q2147.9 1483.71 2145.4 1482.9 L2145.4 1478.5 Q2147.76 1479.73 2150.05 1480.36 Q2152.34 1480.96 2154.59 1480.96 Q2157.59 1480.96 2159.22 1479.94 Q2160.84 1478.9 2160.84 1477.02 Q2160.84 1475.29 2159.65 1474.36 Q2158.5 1473.44 2154.54 1472.58 L2153.06 1472.23 Q2149.1 1471.4 2147.34 1469.69 Q2145.58 1467.95 2145.58 1464.94 Q2145.58 1461.28 2148.17 1459.29 Q2150.77 1457.3 2155.53 1457.3 Q2157.9 1457.3 2159.98 1457.65 Q2162.06 1458 2163.82 1458.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2087.06 1449.29 L2091.74 1449.29 L2091.74 1481.44 Q2091.74 1487.69 2089.35 1490.52 Q2086.99 1493.34 2081.74 1493.34 L2079.95 1493.34 L2079.95 1489.41 L2081.41 1489.41 Q2084.51 1489.41 2085.79 1487.67 Q2087.06 1485.94 2087.06 1481.44 L2087.06 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2110.9 1460.91 Q2107.48 1460.91 2105.49 1463.6 Q2103.49 1466.26 2103.49 1470.91 Q2103.49 1475.56 2105.46 1478.25 Q2107.45 1480.91 2110.9 1480.91 Q2114.3 1480.91 2116.3 1478.23 Q2118.29 1475.54 2118.29 1470.91 Q2118.29 1466.31 2116.3 1463.62 Q2114.3 1460.91 2110.9 1460.91 M2110.9 1457.3 Q2116.46 1457.3 2119.63 1460.91 Q2122.8 1464.52 2122.8 1470.91 Q2122.8 1477.28 2119.63 1480.91 Q2116.46 1484.52 2110.9 1484.52 Q2105.32 1484.52 2102.15 1480.91 Q2099 1477.28 2099 1470.91 Q2099 1464.52 2102.15 1460.91 Q2105.32 1457.3 2110.9 1457.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2151.41 1468.2 L2151.41 1483.85 L2147.15 1483.85 L2147.15 1468.34 Q2147.15 1464.66 2145.72 1462.83 Q2144.28 1461 2141.41 1461 Q2137.96 1461 2135.97 1463.2 Q2133.98 1465.4 2133.98 1469.2 L2133.98 1483.85 L2129.7 1483.85 L2129.7 1457.93 L2133.98 1457.93 L2133.98 1461.95 Q2135.51 1459.62 2137.57 1458.46 Q2139.65 1457.3 2142.36 1457.3 Q2146.83 1457.3 2149.12 1460.08 Q2151.41 1462.83 2151.41 1468.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2159.74 1447.83 L2164.03 1447.83 L2164.03 1469.11 L2176.73 1457.93 L2182.17 1457.93 L2168.42 1470.06 L2182.75 1483.85 L2177.2 1483.85 L2164.03 1471.19 L2164.03 1483.85 L2159.74 1483.85 L2159.74 1447.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2208.91 1468.2 L2208.91 1483.85 L2204.65 1483.85 L2204.65 1468.34 Q2204.65 1464.66 2203.22 1462.83 Q2201.78 1461 2198.91 1461 Q2195.46 1461 2193.47 1463.2 Q2191.48 1465.4 2191.48 1469.2 L2191.48 1483.85 L2187.2 1483.85 L2187.2 1447.83 L2191.48 1447.83 L2191.48 1461.95 Q2193.01 1459.62 2195.07 1458.46 Q2197.15 1457.3 2199.86 1457.3 Q2204.33 1457.3 2206.62 1460.08 Q2208.91 1462.83 2208.91 1468.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2239.58 1469.82 L2239.58 1471.91 L2220 1471.91 Q2220.28 1476.31 2222.64 1478.62 Q2225.02 1480.91 2229.26 1480.91 Q2231.71 1480.91 2234 1480.31 Q2236.32 1479.71 2238.59 1478.5 L2238.59 1482.53 Q2236.29 1483.5 2233.89 1484.01 Q2231.48 1484.52 2229 1484.52 Q2222.8 1484.52 2219.17 1480.91 Q2215.55 1477.3 2215.55 1471.14 Q2215.55 1464.78 2218.98 1461.05 Q2222.43 1457.3 2228.26 1457.3 Q2233.49 1457.3 2236.53 1460.68 Q2239.58 1464.04 2239.58 1469.82 M2235.32 1468.57 Q2235.28 1465.08 2233.35 1463 Q2231.46 1460.91 2228.31 1460.91 Q2224.74 1460.91 2222.59 1462.93 Q2220.46 1464.94 2220.14 1468.6 L2235.32 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2268.75 1469.82 L2268.75 1471.91 L2249.17 1471.91 Q2249.44 1476.31 2251.8 1478.62 Q2254.19 1480.91 2258.42 1480.91 Q2260.88 1480.91 2263.17 1480.31 Q2265.48 1479.71 2267.75 1478.5 L2267.75 1482.53 Q2265.46 1483.5 2263.05 1484.01 Q2260.65 1484.52 2258.17 1484.52 Q2251.97 1484.52 2248.33 1480.91 Q2244.72 1477.3 2244.72 1471.14 Q2244.72 1464.78 2248.15 1461.05 Q2251.6 1457.3 2257.43 1457.3 Q2262.66 1457.3 2265.69 1460.68 Q2268.75 1464.04 2268.75 1469.82 M2264.49 1468.57 Q2264.44 1465.08 2262.52 1463 Q2260.62 1460.91 2257.48 1460.91 Q2253.91 1460.91 2251.76 1462.93 Q2249.63 1464.94 2249.3 1468.6 L2264.49 1468.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2290.76 1461.91 Q2290.04 1461.49 2289.19 1461.31 Q2288.35 1461.1 2287.34 1461.1 Q2283.73 1461.1 2281.78 1463.46 Q2279.86 1465.8 2279.86 1470.19 L2279.86 1483.85 L2275.58 1483.85 L2275.58 1457.93 L2279.86 1457.93 L2279.86 1461.95 Q2281.2 1459.59 2283.35 1458.46 Q2285.51 1457.3 2288.59 1457.3 Q2289.03 1457.3 2289.56 1457.37 Q2290.09 1457.42 2290.74 1457.53 L2290.76 1461.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1208.69 1520.52 L1248.89 1520.52 L1248.89 1525.93 L1232.02 1525.93 L1232.02 1568.04 L1225.56 1568.04 L1225.56 1525.93 L1208.69 1525.93 L1208.69 1520.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1252.83 1532.4 L1258.69 1532.4 L1258.69 1568.04 L1252.83 1568.04 L1252.83 1532.4 M1252.83 1518.52 L1258.69 1518.52 L1258.69 1525.93 L1252.83 1525.93 L1252.83 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1276.74 1522.27 L1276.74 1532.4 L1288.8 1532.4 L1288.8 1536.95 L1276.74 1536.95 L1276.74 1556.3 Q1276.74 1560.66 1277.91 1561.9 Q1279.12 1563.14 1282.78 1563.14 L1288.8 1563.14 L1288.8 1568.04 L1282.78 1568.04 Q1276 1568.04 1273.43 1565.53 Q1270.85 1562.98 1270.85 1556.3 L1270.85 1536.95 L1266.55 1536.95 L1266.55 1532.4 L1270.85 1532.4 L1270.85 1522.27 L1276.74 1522.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1296.5 1518.52 L1302.36 1518.52 L1302.36 1568.04 L1296.5 1568.04 L1296.5 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1345.1 1548.76 L1345.1 1551.62 L1318.18 1551.62 Q1318.56 1557.67 1321.81 1560.85 Q1325.08 1564 1330.91 1564 Q1334.28 1564 1337.43 1563.17 Q1340.62 1562.35 1343.74 1560.69 L1343.74 1566.23 Q1340.58 1567.57 1337.27 1568.27 Q1333.96 1568.97 1330.56 1568.97 Q1322.03 1568.97 1317.03 1564 Q1312.07 1559.04 1312.07 1550.57 Q1312.07 1541.82 1316.78 1536.69 Q1321.52 1531.54 1329.54 1531.54 Q1336.73 1531.54 1340.9 1536.18 Q1345.1 1540.8 1345.1 1548.76 M1339.25 1547.04 Q1339.18 1542.23 1336.54 1539.37 Q1333.93 1536.5 1329.6 1536.5 Q1324.7 1536.5 1321.74 1539.27 Q1318.81 1542.04 1318.37 1547.07 L1339.25 1547.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1377.44 1533.45 L1377.44 1538.98 Q1374.96 1537.71 1372.29 1537.07 Q1369.61 1536.44 1366.75 1536.44 Q1362.39 1536.44 1360.19 1537.77 Q1358.03 1539.11 1358.03 1541.79 Q1358.03 1543.82 1359.59 1545 Q1361.15 1546.15 1365.86 1547.2 L1367.86 1547.64 Q1374.1 1548.98 1376.71 1551.43 Q1379.35 1553.85 1379.35 1558.21 Q1379.35 1563.17 1375.4 1566.07 Q1371.49 1568.97 1364.62 1568.97 Q1361.75 1568.97 1358.63 1568.39 Q1355.54 1567.85 1352.11 1566.74 L1352.11 1560.69 Q1355.35 1562.38 1358.5 1563.24 Q1361.66 1564.07 1364.74 1564.07 Q1368.88 1564.07 1371.11 1562.66 Q1373.34 1561.23 1373.34 1558.65 Q1373.34 1556.27 1371.71 1554.99 Q1370.12 1553.72 1364.68 1552.54 L1362.64 1552.07 Q1357.2 1550.92 1354.78 1548.56 Q1352.36 1546.18 1352.36 1542.04 Q1352.36 1537.01 1355.93 1534.27 Q1359.49 1531.54 1366.05 1531.54 Q1369.29 1531.54 1372.16 1532.01 Q1375.02 1532.49 1377.44 1533.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,1386.4 2352.76,1386.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,1149.69 2352.76,1149.69 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,912.983 2352.76,912.983 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,676.276 2352.76,676.276 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,439.57 2352.76,439.57 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  235.283,202.863 2352.76,202.863 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,1423.18 235.283,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,1386.4 260.693,1386.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,1149.69 260.693,1149.69 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,912.983 260.693,912.983 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,676.276 260.693,676.276 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,439.57 260.693,439.57 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip170)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  235.283,202.863 260.693,202.863 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 1372.19 Q183.727 1372.19 181.899 1375.76 Q180.093 1379.3 180.093 1386.43 Q180.093 1393.54 181.899 1397.1 Q183.727 1400.64 187.338 1400.64 Q190.973 1400.64 192.778 1397.1 Q194.607 1393.54 194.607 1386.43 Q194.607 1379.3 192.778 1375.76 Q190.973 1372.19 187.338 1372.19 M187.338 1368.49 Q193.149 1368.49 196.204 1373.1 Q199.283 1377.68 199.283 1386.43 Q199.283 1395.16 196.204 1399.76 Q193.149 1404.35 187.338 1404.35 Q181.528 1404.35 178.45 1399.76 Q175.394 1395.16 175.394 1386.43 Q175.394 1377.68 178.45 1373.1 Q181.528 1368.49 187.338 1368.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M117.825 1163.03 L125.464 1163.03 L125.464 1136.67 L117.154 1138.34 L117.154 1134.08 L125.418 1132.41 L130.093 1132.41 L130.093 1163.03 L137.732 1163.03 L137.732 1166.97 L117.825 1166.97 L117.825 1163.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M157.177 1135.49 Q153.566 1135.49 151.737 1139.05 Q149.931 1142.59 149.931 1149.72 Q149.931 1156.83 151.737 1160.4 Q153.566 1163.94 157.177 1163.94 Q160.811 1163.94 162.616 1160.4 Q164.445 1156.83 164.445 1149.72 Q164.445 1142.59 162.616 1139.05 Q160.811 1135.49 157.177 1135.49 M157.177 1131.78 Q162.987 1131.78 166.042 1136.39 Q169.121 1140.97 169.121 1149.72 Q169.121 1158.45 166.042 1163.06 Q162.987 1167.64 157.177 1167.64 Q151.366 1167.64 148.288 1163.06 Q145.232 1158.45 145.232 1149.72 Q145.232 1140.97 148.288 1136.39 Q151.366 1131.78 157.177 1131.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 1135.49 Q183.727 1135.49 181.899 1139.05 Q180.093 1142.59 180.093 1149.72 Q180.093 1156.83 181.899 1160.4 Q183.727 1163.94 187.338 1163.94 Q190.973 1163.94 192.778 1160.4 Q194.607 1156.83 194.607 1149.72 Q194.607 1142.59 192.778 1139.05 Q190.973 1135.49 187.338 1135.49 M187.338 1131.78 Q193.149 1131.78 196.204 1136.39 Q199.283 1140.97 199.283 1149.72 Q199.283 1158.45 196.204 1163.06 Q193.149 1167.64 187.338 1167.64 Q181.528 1167.64 178.45 1163.06 Q175.394 1158.45 175.394 1149.72 Q175.394 1140.97 178.45 1136.39 Q181.528 1131.78 187.338 1131.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M121.043 926.328 L137.362 926.328 L137.362 930.263 L115.418 930.263 L115.418 926.328 Q118.08 923.573 122.663 918.943 Q127.269 914.291 128.45 912.948 Q130.695 910.425 131.575 908.689 Q132.478 906.93 132.478 905.24 Q132.478 902.485 130.533 900.749 Q128.612 899.013 125.51 899.013 Q123.311 899.013 120.857 899.777 Q118.427 900.541 115.649 902.092 L115.649 897.369 Q118.473 896.235 120.927 895.656 Q123.38 895.078 125.418 895.078 Q130.788 895.078 133.982 897.763 Q137.177 900.448 137.177 904.939 Q137.177 907.068 136.367 908.99 Q135.579 910.888 133.473 913.48 Q132.894 914.152 129.792 917.369 Q126.691 920.564 121.043 926.328 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M157.177 898.781 Q153.566 898.781 151.737 902.346 Q149.931 905.888 149.931 913.018 Q149.931 920.124 151.737 923.689 Q153.566 927.23 157.177 927.23 Q160.811 927.23 162.616 923.689 Q164.445 920.124 164.445 913.018 Q164.445 905.888 162.616 902.346 Q160.811 898.781 157.177 898.781 M157.177 895.078 Q162.987 895.078 166.042 899.684 Q169.121 904.268 169.121 913.018 Q169.121 921.744 166.042 926.351 Q162.987 930.934 157.177 930.934 Q151.366 930.934 148.288 926.351 Q145.232 921.744 145.232 913.018 Q145.232 904.268 148.288 899.684 Q151.366 895.078 157.177 895.078 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 898.781 Q183.727 898.781 181.899 902.346 Q180.093 905.888 180.093 913.018 Q180.093 920.124 181.899 923.689 Q183.727 927.23 187.338 927.23 Q190.973 927.23 192.778 923.689 Q194.607 920.124 194.607 913.018 Q194.607 905.888 192.778 902.346 Q190.973 898.781 187.338 898.781 M187.338 895.078 Q193.149 895.078 196.204 899.684 Q199.283 904.268 199.283 913.018 Q199.283 921.744 196.204 926.351 Q193.149 930.934 187.338 930.934 Q181.528 930.934 178.45 926.351 Q175.394 921.744 175.394 913.018 Q175.394 904.268 178.45 899.684 Q181.528 895.078 187.338 895.078 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M131.181 674.922 Q134.538 675.64 136.413 677.908 Q138.311 680.177 138.311 683.51 Q138.311 688.626 134.792 691.427 Q131.274 694.228 124.793 694.228 Q122.617 694.228 120.302 693.788 Q118.01 693.371 115.556 692.515 L115.556 688.001 Q117.501 689.135 119.816 689.714 Q122.13 690.292 124.654 690.292 Q129.052 690.292 131.343 688.556 Q133.658 686.82 133.658 683.51 Q133.658 680.455 131.505 678.742 Q129.376 677.006 125.556 677.006 L121.529 677.006 L121.529 673.163 L125.742 673.163 Q129.191 673.163 131.019 671.797 Q132.848 670.408 132.848 667.816 Q132.848 665.154 130.95 663.742 Q129.075 662.307 125.556 662.307 Q123.635 662.307 121.436 662.723 Q119.237 663.14 116.598 664.019 L116.598 659.853 Q119.26 659.112 121.575 658.742 Q123.913 658.371 125.973 658.371 Q131.297 658.371 134.399 660.802 Q137.501 663.209 137.501 667.33 Q137.501 670.2 135.857 672.191 Q134.214 674.158 131.181 674.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M157.177 662.075 Q153.566 662.075 151.737 665.64 Q149.931 669.181 149.931 676.311 Q149.931 683.418 151.737 686.982 Q153.566 690.524 157.177 690.524 Q160.811 690.524 162.616 686.982 Q164.445 683.418 164.445 676.311 Q164.445 669.181 162.616 665.64 Q160.811 662.075 157.177 662.075 M157.177 658.371 Q162.987 658.371 166.042 662.978 Q169.121 667.561 169.121 676.311 Q169.121 685.038 166.042 689.644 Q162.987 694.228 157.177 694.228 Q151.366 694.228 148.288 689.644 Q145.232 685.038 145.232 676.311 Q145.232 667.561 148.288 662.978 Q151.366 658.371 157.177 658.371 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 662.075 Q183.727 662.075 181.899 665.64 Q180.093 669.181 180.093 676.311 Q180.093 683.418 181.899 686.982 Q183.727 690.524 187.338 690.524 Q190.973 690.524 192.778 686.982 Q194.607 683.418 194.607 676.311 Q194.607 669.181 192.778 665.64 Q190.973 662.075 187.338 662.075 M187.338 658.371 Q193.149 658.371 196.204 662.978 Q199.283 667.561 199.283 676.311 Q199.283 685.038 196.204 689.644 Q193.149 694.228 187.338 694.228 Q181.528 694.228 178.45 689.644 Q175.394 685.038 175.394 676.311 Q175.394 667.561 178.45 662.978 Q181.528 658.371 187.338 658.371 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M129.862 426.364 L118.056 444.813 L129.862 444.813 L129.862 426.364 M128.635 422.29 L134.515 422.29 L134.515 444.813 L139.445 444.813 L139.445 448.702 L134.515 448.702 L134.515 456.85 L129.862 456.85 L129.862 448.702 L114.26 448.702 L114.26 444.188 L128.635 422.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M157.177 425.369 Q153.566 425.369 151.737 428.933 Q149.931 432.475 149.931 439.605 Q149.931 446.711 151.737 450.276 Q153.566 453.818 157.177 453.818 Q160.811 453.818 162.616 450.276 Q164.445 446.711 164.445 439.605 Q164.445 432.475 162.616 428.933 Q160.811 425.369 157.177 425.369 M157.177 421.665 Q162.987 421.665 166.042 426.271 Q169.121 430.855 169.121 439.605 Q169.121 448.331 166.042 452.938 Q162.987 457.521 157.177 457.521 Q151.366 457.521 148.288 452.938 Q145.232 448.331 145.232 439.605 Q145.232 430.855 148.288 426.271 Q151.366 421.665 157.177 421.665 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 425.369 Q183.727 425.369 181.899 428.933 Q180.093 432.475 180.093 439.605 Q180.093 446.711 181.899 450.276 Q183.727 453.818 187.338 453.818 Q190.973 453.818 192.778 450.276 Q194.607 446.711 194.607 439.605 Q194.607 432.475 192.778 428.933 Q190.973 425.369 187.338 425.369 M187.338 421.665 Q193.149 421.665 196.204 426.271 Q199.283 430.855 199.283 439.605 Q199.283 448.331 196.204 452.938 Q193.149 457.521 187.338 457.521 Q181.528 457.521 178.45 452.938 Q175.394 448.331 175.394 439.605 Q175.394 430.855 178.45 426.271 Q181.528 421.665 187.338 421.665 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M117.061 185.583 L135.417 185.583 L135.417 189.519 L121.343 189.519 L121.343 197.991 Q122.362 197.644 123.38 197.482 Q124.399 197.296 125.418 197.296 Q131.205 197.296 134.584 200.468 Q137.964 203.639 137.964 209.056 Q137.964 214.634 134.492 217.736 Q131.019 220.815 124.7 220.815 Q122.524 220.815 120.255 220.444 Q118.01 220.074 115.603 219.333 L115.603 214.634 Q117.686 215.768 119.908 216.324 Q122.13 216.88 124.607 216.88 Q128.612 216.88 130.95 214.773 Q133.288 212.667 133.288 209.056 Q133.288 205.444 130.95 203.338 Q128.612 201.232 124.607 201.232 Q122.732 201.232 120.857 201.648 Q119.006 202.065 117.061 202.944 L117.061 185.583 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M157.177 188.662 Q153.566 188.662 151.737 192.227 Q149.931 195.769 149.931 202.898 Q149.931 210.005 151.737 213.569 Q153.566 217.111 157.177 217.111 Q160.811 217.111 162.616 213.569 Q164.445 210.005 164.445 202.898 Q164.445 195.769 162.616 192.227 Q160.811 188.662 157.177 188.662 M157.177 184.958 Q162.987 184.958 166.042 189.565 Q169.121 194.148 169.121 202.898 Q169.121 211.625 166.042 216.231 Q162.987 220.815 157.177 220.815 Q151.366 220.815 148.288 216.231 Q145.232 211.625 145.232 202.898 Q145.232 194.148 148.288 189.565 Q151.366 184.958 157.177 184.958 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M187.338 188.662 Q183.727 188.662 181.899 192.227 Q180.093 195.769 180.093 202.898 Q180.093 210.005 181.899 213.569 Q183.727 217.111 187.338 217.111 Q190.973 217.111 192.778 213.569 Q194.607 210.005 194.607 202.898 Q194.607 195.769 192.778 192.227 Q190.973 188.662 187.338 188.662 M187.338 184.958 Q193.149 184.958 196.204 189.565 Q199.283 194.148 199.283 202.898 Q199.283 211.625 196.204 216.231 Q193.149 220.815 187.338 220.815 Q181.528 220.815 178.45 216.231 Q175.394 211.625 175.394 202.898 Q175.394 194.148 178.45 189.565 Q181.528 184.958 187.338 184.958 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M16.4842 1136.38 L16.4842 1127.72 L56.238 1106.65 L16.4842 1106.65 L16.4842 1100.41 L64.0042 1100.41 L64.0042 1109.07 L24.2503 1130.14 L64.0042 1130.14 L64.0042 1136.38 L16.4842 1136.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M49.9359 1088.48 L28.3562 1088.48 L28.3562 1082.62 L49.7131 1082.62 Q54.7739 1082.62 57.3202 1080.65 Q59.8346 1078.67 59.8346 1074.73 Q59.8346 1069.98 56.8109 1067.25 Q53.7872 1064.48 48.5673 1064.48 L28.3562 1064.48 L28.3562 1058.62 L64.0042 1058.62 L64.0042 1064.48 L58.5296 1064.48 Q61.7762 1066.61 63.3676 1069.44 Q64.9272 1072.24 64.9272 1075.97 Q64.9272 1082.11 61.1078 1085.29 Q57.2883 1088.48 49.9359 1088.48 M27.4968 1073.74 L27.4968 1073.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M35.1993 1018.8 Q31.2526 1016.61 29.3747 1013.55 Q27.4968 1010.5 27.4968 1006.36 Q27.4968 1000.79 31.4117 997.765 Q35.2948 994.741 42.4881 994.741 L64.0042 994.741 L64.0042 1000.63 L42.679 1000.63 Q37.5546 1000.63 35.072 1002.44 Q32.5894 1004.26 32.5894 1007.98 Q32.5894 1012.53 35.6131 1015.18 Q38.6368 1017.82 43.8567 1017.82 L64.0042 1017.82 L64.0042 1023.71 L42.679 1023.71 Q37.5228 1023.71 35.072 1025.52 Q32.5894 1027.33 32.5894 1031.12 Q32.5894 1035.61 35.6449 1038.25 Q38.6686 1040.89 43.8567 1040.89 L64.0042 1040.89 L64.0042 1046.78 L28.3562 1046.78 L28.3562 1040.89 L33.8944 1040.89 Q30.616 1038.89 29.0564 1036.09 Q27.4968 1033.29 27.4968 1029.43 Q27.4968 1025.55 29.4702 1022.85 Q31.4436 1020.11 35.1993 1018.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M46.212 957.47 Q39.7508 957.47 36.0905 960.144 Q32.3984 962.785 32.3984 967.432 Q32.3984 972.079 36.0905 974.753 Q39.7508 977.395 46.212 977.395 Q52.6732 977.395 56.3653 974.753 Q60.0256 972.079 60.0256 967.432 Q60.0256 962.785 56.3653 960.144 Q52.6732 957.47 46.212 957.47 M33.7671 977.395 Q30.5842 975.549 29.0564 972.748 Q27.4968 969.915 27.4968 966 Q27.4968 959.507 32.6531 955.465 Q37.8093 951.391 46.212 951.391 Q54.6147 951.391 59.771 955.465 Q64.9272 959.507 64.9272 966 Q64.9272 969.915 63.3994 972.748 Q61.8398 975.549 58.657 977.395 L64.0042 977.395 L64.0042 983.283 L14.479 983.283 L14.479 977.395 L33.7671 977.395 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M44.7161 911.191 L47.5806 911.191 L47.5806 938.118 Q53.6281 937.736 56.8109 934.49 Q59.9619 931.211 59.9619 925.387 Q59.9619 922.013 59.1344 918.862 Q58.3069 915.679 56.6518 912.56 L62.1899 912.56 Q63.5267 915.711 64.227 919.021 Q64.9272 922.331 64.9272 925.737 Q64.9272 934.267 59.9619 939.264 Q54.9967 944.229 46.5303 944.229 Q37.7774 944.229 32.6531 939.519 Q27.4968 934.776 27.4968 926.755 Q27.4968 919.562 32.1438 915.393 Q36.7589 911.191 44.7161 911.191 M42.9973 917.048 Q38.1912 917.111 35.3266 919.753 Q32.4621 922.363 32.4621 926.692 Q32.4621 931.593 35.2312 934.553 Q38.0002 937.482 43.0292 937.927 L42.9973 917.048 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M33.8307 880.922 Q33.2578 881.909 33.0032 883.087 Q32.7167 884.233 32.7167 885.633 Q32.7167 890.598 35.9632 893.272 Q39.1779 895.914 45.2253 895.914 L64.0042 895.914 L64.0042 901.802 L28.3562 901.802 L28.3562 895.914 L33.8944 895.914 Q30.6479 894.068 29.0883 891.107 Q27.4968 888.147 27.4968 883.914 Q27.4968 883.309 27.5923 882.577 Q27.656 881.845 27.8151 880.954 L33.8307 880.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M32.4621 840.245 Q32.4621 844.956 36.1542 847.693 Q39.8145 850.431 46.212 850.431 Q52.6095 850.431 56.3017 847.725 Q59.9619 844.988 59.9619 840.245 Q59.9619 835.567 56.2698 832.829 Q52.5777 830.092 46.212 830.092 Q39.8781 830.092 36.186 832.829 Q32.4621 835.567 32.4621 840.245 M27.4968 840.245 Q27.4968 832.607 32.4621 828.246 Q37.4273 823.886 46.212 823.886 Q54.9649 823.886 59.9619 828.246 Q64.9272 832.607 64.9272 840.245 Q64.9272 847.916 59.9619 852.277 Q54.9649 856.605 46.212 856.605 Q37.4273 856.605 32.4621 852.277 Q27.4968 847.916 27.4968 840.245 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M14.479 796.131 L19.3487 796.131 L19.3487 801.733 Q19.3487 804.884 20.6219 806.125 Q21.895 807.335 25.2052 807.335 L28.3562 807.335 L28.3562 797.691 L32.9077 797.691 L32.9077 807.335 L64.0042 807.335 L64.0042 813.223 L32.9077 813.223 L32.9077 818.825 L28.3562 818.825 L28.3562 813.223 L25.8736 813.223 Q19.9216 813.223 17.2162 810.454 Q14.479 807.685 14.479 801.669 L14.479 796.131 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M32.4621 756.696 Q32.4621 761.406 36.1542 764.143 Q39.8145 766.881 46.212 766.881 Q52.6095 766.881 56.3017 764.175 Q59.9619 761.438 59.9619 756.696 Q59.9619 752.017 56.2698 749.28 Q52.5777 746.542 46.212 746.542 Q39.8781 746.542 36.186 749.28 Q32.4621 752.017 32.4621 756.696 M27.4968 756.696 Q27.4968 749.057 32.4621 744.696 Q37.4273 740.336 46.212 740.336 Q54.9649 740.336 59.9619 744.696 Q64.9272 749.057 64.9272 756.696 Q64.9272 764.366 59.9619 768.727 Q54.9649 773.055 46.212 773.055 Q37.4273 773.055 32.4621 768.727 Q27.4968 764.366 27.4968 756.696 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M29.7248 704.974 L35.1993 704.974 Q33.8307 707.457 33.1623 709.971 Q32.4621 712.454 32.4621 715 Q32.4621 720.698 36.0905 723.849 Q39.6872 727 46.212 727 Q52.7369 727 56.3653 723.849 Q59.9619 720.698 59.9619 715 Q59.9619 712.454 59.2935 709.971 Q58.5933 707.457 57.2247 704.974 L62.6355 704.974 Q63.7814 707.425 64.3543 710.067 Q64.9272 712.677 64.9272 715.637 Q64.9272 723.689 59.8664 728.432 Q54.8057 733.174 46.212 733.174 Q37.491 733.174 32.4939 728.4 Q27.4968 723.594 27.4968 715.255 Q27.4968 712.549 28.0697 709.971 Q28.6108 707.393 29.7248 704.974 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M29.7248 669.135 L35.1993 669.135 Q33.8307 671.618 33.1623 674.132 Q32.4621 676.615 32.4621 679.161 Q32.4621 684.859 36.0905 688.01 Q39.6872 691.161 46.212 691.161 Q52.7369 691.161 56.3653 688.01 Q59.9619 684.859 59.9619 679.161 Q59.9619 676.615 59.2935 674.132 Q58.5933 671.618 57.2247 669.135 L62.6355 669.135 Q63.7814 671.586 64.3543 674.228 Q64.9272 676.838 64.9272 679.798 Q64.9272 687.85 59.8664 692.593 Q54.8057 697.335 46.212 697.335 Q37.491 697.335 32.4939 692.561 Q27.4968 687.755 27.4968 679.416 Q27.4968 676.71 28.0697 674.132 Q28.6108 671.554 29.7248 669.135 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M49.9359 659.555 L28.3562 659.555 L28.3562 653.698 L49.7131 653.698 Q54.7739 653.698 57.3202 651.725 Q59.8346 649.752 59.8346 645.805 Q59.8346 641.063 56.8109 638.325 Q53.7872 635.556 48.5673 635.556 L28.3562 635.556 L28.3562 629.7 L64.0042 629.7 L64.0042 635.556 L58.5296 635.556 Q61.7762 637.689 63.3676 640.521 Q64.9272 643.322 64.9272 647.046 Q64.9272 653.189 61.1078 656.372 Q57.2883 659.555 49.9359 659.555 M27.4968 644.818 L27.4968 644.818 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M33.8307 596.98 Q33.2578 597.967 33.0032 599.144 Q32.7167 600.29 32.7167 601.691 Q32.7167 606.656 35.9632 609.329 Q39.1779 611.971 45.2253 611.971 L64.0042 611.971 L64.0042 617.86 L28.3562 617.86 L28.3562 611.971 L33.8944 611.971 Q30.6479 610.125 29.0883 607.165 Q27.4968 604.205 27.4968 599.972 Q27.4968 599.367 27.5923 598.635 Q27.656 597.903 27.8151 597.012 L33.8307 596.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M44.7161 561.778 L47.5806 561.778 L47.5806 588.705 Q53.6281 588.323 56.8109 585.076 Q59.9619 581.798 59.9619 575.973 Q59.9619 572.599 59.1344 569.448 Q58.3069 566.265 56.6518 563.146 L62.1899 563.146 Q63.5267 566.297 64.227 569.607 Q64.9272 572.918 64.9272 576.323 Q64.9272 584.853 59.9619 589.85 Q54.9967 594.816 46.5303 594.816 Q37.7774 594.816 32.6531 590.105 Q27.4968 585.363 27.4968 577.342 Q27.4968 570.149 32.1438 565.979 Q36.7589 561.778 44.7161 561.778 M42.9973 567.634 Q38.1912 567.698 35.3266 570.34 Q32.4621 572.949 32.4621 577.278 Q32.4621 582.18 35.2312 585.14 Q38.0002 588.068 43.0292 588.514 L42.9973 567.634 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M42.4881 522.533 L64.0042 522.533 L64.0042 528.39 L42.679 528.39 Q37.6183 528.39 35.1038 530.363 Q32.5894 532.336 32.5894 536.283 Q32.5894 541.025 35.6131 543.763 Q38.6368 546.5 43.8567 546.5 L64.0042 546.5 L64.0042 552.388 L28.3562 552.388 L28.3562 546.5 L33.8944 546.5 Q30.6797 544.399 29.0883 541.567 Q27.4968 538.702 27.4968 534.978 Q27.4968 528.835 31.3163 525.684 Q35.1038 522.533 42.4881 522.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M29.7248 485.198 L35.1993 485.198 Q33.8307 487.681 33.1623 490.195 Q32.4621 492.678 32.4621 495.224 Q32.4621 500.921 36.0905 504.073 Q39.6872 507.224 46.212 507.224 Q52.7369 507.224 56.3653 504.073 Q59.9619 500.921 59.9619 495.224 Q59.9619 492.678 59.2935 490.195 Q58.5933 487.681 57.2247 485.198 L62.6355 485.198 Q63.7814 487.649 64.3543 490.291 Q64.9272 492.901 64.9272 495.861 Q64.9272 503.913 59.8664 508.656 Q54.8057 513.398 46.212 513.398 Q37.491 513.398 32.4939 508.624 Q27.4968 503.818 27.4968 495.479 Q27.4968 492.773 28.0697 490.195 Q28.6108 487.617 29.7248 485.198 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M44.7161 444.521 L47.5806 444.521 L47.5806 471.448 Q53.6281 471.066 56.8109 467.82 Q59.9619 464.541 59.9619 458.717 Q59.9619 455.343 59.1344 452.192 Q58.3069 449.009 56.6518 445.89 L62.1899 445.89 Q63.5267 449.041 64.227 452.351 Q64.9272 455.661 64.9272 459.067 Q64.9272 467.597 59.9619 472.594 Q54.9967 477.559 46.5303 477.559 Q37.7774 477.559 32.6531 472.849 Q27.4968 468.106 27.4968 460.085 Q27.4968 452.892 32.1438 448.723 Q36.7589 444.521 44.7161 444.521 M42.9973 450.378 Q38.1912 450.441 35.3266 453.083 Q32.4621 455.693 32.4621 460.022 Q32.4621 464.923 35.2312 467.883 Q38.0002 470.812 43.0292 471.257 L42.9973 450.378 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M29.4065 412.184 L34.9447 412.184 Q33.6716 414.666 33.035 417.34 Q32.3984 420.013 32.3984 422.878 Q32.3984 427.238 33.7352 429.435 Q35.072 431.599 37.7456 431.599 Q39.7826 431.599 40.9603 430.039 Q42.1061 428.48 43.1565 423.769 L43.6021 421.764 Q44.9389 415.526 47.3897 412.916 Q49.8086 410.274 54.1691 410.274 Q59.1344 410.274 62.0308 414.221 Q64.9272 418.135 64.9272 425.01 Q64.9272 427.875 64.3543 430.994 Q63.8132 434.082 62.6992 437.519 L56.6518 437.519 Q58.3387 434.273 59.198 431.122 Q60.0256 427.97 60.0256 424.883 Q60.0256 420.745 58.6251 418.517 Q57.1929 416.289 54.6147 416.289 Q52.2276 416.289 50.9545 417.913 Q49.6813 419.504 48.5037 424.947 L48.0262 426.984 Q46.8804 432.426 44.5251 434.845 Q42.138 437.264 38.0002 437.264 Q32.9713 437.264 30.2341 433.7 Q27.4968 430.135 27.4968 423.578 Q27.4968 420.332 27.9743 417.467 Q28.4517 414.603 29.4065 412.184 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M589.709 17.6457 Q580.797 17.6457 575.531 24.2892 Q570.305 30.9327 570.305 42.3968 Q570.305 53.8203 575.531 60.4638 Q580.797 67.1073 589.709 67.1073 Q598.621 67.1073 603.806 60.4638 Q609.032 53.8203 609.032 42.3968 Q609.032 30.9327 603.806 24.2892 Q598.621 17.6457 589.709 17.6457 M589.709 11.0023 Q602.429 11.0023 610.044 19.5497 Q617.66 28.0566 617.66 42.3968 Q617.66 56.6965 610.044 65.2439 Q602.429 73.7508 589.709 73.7508 Q576.949 73.7508 569.292 65.2439 Q561.677 56.737 561.677 42.3968 Q561.677 28.0566 569.292 19.5497 Q576.949 11.0023 589.709 11.0023 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M662.787 28.9478 L662.787 35.9153 Q659.628 34.1734 656.427 33.3227 Q653.268 32.4315 650.027 32.4315 Q642.776 32.4315 638.765 37.0496 Q634.755 41.6271 634.755 49.9314 Q634.755 58.2358 638.765 62.8538 Q642.776 67.4314 650.027 67.4314 Q653.268 67.4314 656.427 66.5807 Q659.628 65.6895 662.787 63.9476 L662.787 70.8341 Q659.668 72.2924 656.306 73.0216 Q652.984 73.7508 649.217 73.7508 Q638.968 73.7508 632.932 67.3098 Q626.896 60.8689 626.896 49.9314 Q626.896 38.832 632.973 32.472 Q639.089 26.1121 649.703 26.1121 Q653.146 26.1121 656.427 26.8413 Q659.709 27.5299 662.787 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M708.4 28.9478 L708.4 35.9153 Q705.241 34.1734 702.04 33.3227 Q698.881 32.4315 695.64 32.4315 Q688.389 32.4315 684.379 37.0496 Q680.368 41.6271 680.368 49.9314 Q680.368 58.2358 684.379 62.8538 Q688.389 67.4314 695.64 67.4314 Q698.881 67.4314 702.04 66.5807 Q705.241 65.6895 708.4 63.9476 L708.4 70.8341 Q705.281 72.2924 701.919 73.0216 Q698.597 73.7508 694.83 73.7508 Q684.581 73.7508 678.545 67.3098 Q672.509 60.8689 672.509 49.9314 Q672.509 38.832 678.586 32.472 Q684.703 26.1121 695.316 26.1121 Q698.759 26.1121 702.04 26.8413 Q705.322 27.5299 708.4 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M720.594 54.671 L720.594 27.2059 L728.047 27.2059 L728.047 54.3874 Q728.047 60.8284 730.559 64.0691 Q733.07 67.2693 738.094 67.2693 Q744.129 67.2693 747.613 63.421 Q751.137 59.5726 751.137 52.9291 L751.137 27.2059 L758.591 27.2059 L758.591 72.576 L751.137 72.576 L751.137 65.6084 Q748.423 69.7404 744.818 71.7658 Q741.253 73.7508 736.514 73.7508 Q728.695 73.7508 724.645 68.8897 Q720.594 64.0286 720.594 54.671 M739.349 26.1121 L739.349 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M800.234 34.1734 Q798.979 33.4443 797.48 33.1202 Q796.021 32.7556 794.239 32.7556 Q787.92 32.7556 784.517 36.8875 Q781.155 40.9789 781.155 48.6757 L781.155 72.576 L773.66 72.576 L773.66 27.2059 L781.155 27.2059 L781.155 34.2544 Q783.504 30.1225 787.272 28.1376 Q791.039 26.1121 796.427 26.1121 Q797.196 26.1121 798.128 26.2337 Q799.06 26.3147 800.194 26.5172 L800.234 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M845.037 48.0275 L845.037 51.6733 L810.767 51.6733 Q811.253 59.3701 815.385 63.421 Q819.557 67.4314 826.97 67.4314 Q831.264 67.4314 835.275 66.3781 Q839.326 65.3249 843.296 63.2184 L843.296 70.267 Q839.285 71.9684 835.072 72.8596 Q830.859 73.7508 826.525 73.7508 Q815.668 73.7508 809.308 67.4314 Q802.989 61.1119 802.989 50.3365 Q802.989 39.1965 808.984 32.6746 Q815.02 26.1121 825.228 26.1121 Q834.384 26.1121 839.69 32.0264 Q845.037 37.9003 845.037 48.0275 M837.584 45.84 Q837.503 39.7232 834.14 36.0774 Q830.819 32.4315 825.31 32.4315 Q819.071 32.4315 815.304 35.9558 Q811.577 39.4801 811.01 45.8805 L837.584 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M894.985 45.1919 L894.985 72.576 L887.531 72.576 L887.531 45.4349 Q887.531 38.994 885.02 35.7938 Q882.508 32.5936 877.485 32.5936 Q871.449 32.5936 867.966 36.4419 Q864.482 40.2903 864.482 46.9338 L864.482 72.576 L856.988 72.576 L856.988 27.2059 L864.482 27.2059 L864.482 34.2544 Q867.155 30.163 870.761 28.1376 Q874.406 26.1121 879.146 26.1121 Q886.964 26.1121 890.975 30.9732 Q894.985 35.7938 894.985 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M942.502 28.9478 L942.502 35.9153 Q939.342 34.1734 936.142 33.3227 Q932.983 32.4315 929.742 32.4315 Q922.491 32.4315 918.48 37.0496 Q914.47 41.6271 914.47 49.9314 Q914.47 58.2358 918.48 62.8538 Q922.491 67.4314 929.742 67.4314 Q932.983 67.4314 936.142 66.5807 Q939.342 65.6895 942.502 63.9476 L942.502 70.8341 Q939.383 72.2924 936.021 73.0216 Q932.699 73.7508 928.932 73.7508 Q918.683 73.7508 912.647 67.3098 Q906.611 60.8689 906.611 49.9314 Q906.611 38.832 912.688 32.472 Q918.804 26.1121 929.418 26.1121 Q932.861 26.1121 936.142 26.8413 Q939.423 27.5299 942.502 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M955.465 27.2059 L962.919 27.2059 L962.919 72.576 L955.465 72.576 L955.465 27.2059 M955.465 9.54393 L962.919 9.54393 L962.919 18.9825 L955.465 18.9825 L955.465 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1017.32 48.0275 L1017.32 51.6733 L983.052 51.6733 Q983.538 59.3701 987.67 63.421 Q991.842 67.4314 999.255 67.4314 Q1003.55 67.4314 1007.56 66.3781 Q1011.61 65.3249 1015.58 63.2184 L1015.58 70.267 Q1011.57 71.9684 1007.36 72.8596 Q1003.14 73.7508 998.81 73.7508 Q987.953 73.7508 981.593 67.4314 Q975.274 61.1119 975.274 50.3365 Q975.274 39.1965 981.269 32.6746 Q987.305 26.1121 997.513 26.1121 Q1006.67 26.1121 1011.98 32.0264 Q1017.32 37.9003 1017.32 48.0275 M1009.87 45.84 Q1009.79 39.7232 1006.43 36.0774 Q1003.1 32.4315 997.594 32.4315 Q991.356 32.4315 987.589 35.9558 Q983.862 39.4801 983.295 45.8805 L1009.87 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1058.48 28.5427 L1058.48 35.5912 Q1055.32 33.9709 1051.92 33.1607 Q1048.51 32.3505 1044.87 32.3505 Q1039.32 32.3505 1036.52 34.0519 Q1033.77 35.7533 1033.77 39.156 Q1033.77 41.7486 1035.75 43.2475 Q1037.74 44.7058 1043.73 46.0426 L1046.29 46.6097 Q1054.23 48.3111 1057.55 51.4303 Q1060.91 54.509 1060.91 60.0587 Q1060.91 66.3781 1055.89 70.0644 Q1050.9 73.7508 1042.15 73.7508 Q1038.51 73.7508 1034.54 73.0216 Q1030.61 72.3329 1026.23 70.9151 L1026.23 63.2184 Q1030.37 65.3654 1034.38 66.4591 Q1038.39 67.5124 1042.32 67.5124 Q1047.58 67.5124 1050.42 65.73 Q1053.25 63.9071 1053.25 60.6258 Q1053.25 57.5877 1051.19 55.9673 Q1049.16 54.3469 1042.24 52.8481 L1039.64 52.2405 Q1032.72 50.7821 1029.64 47.7845 Q1026.56 44.7463 1026.56 39.4801 Q1026.56 33.0797 1031.1 29.5959 Q1035.63 26.1121 1043.98 26.1121 Q1048.11 26.1121 1051.76 26.7198 Q1055.4 27.3274 1058.48 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1116.73 32.4315 Q1110.74 32.4315 1107.25 37.1306 Q1103.77 41.7891 1103.77 49.9314 Q1103.77 58.0738 1107.21 62.7728 Q1110.7 67.4314 1116.73 67.4314 Q1122.69 67.4314 1126.17 62.7323 Q1129.65 58.0333 1129.65 49.9314 Q1129.65 41.8701 1126.17 37.1711 Q1122.69 32.4315 1116.73 32.4315 M1116.73 26.1121 Q1126.45 26.1121 1132 32.4315 Q1137.55 38.7509 1137.55 49.9314 Q1137.55 61.0714 1132 67.4314 Q1126.45 73.7508 1116.73 73.7508 Q1106.97 73.7508 1101.42 67.4314 Q1095.91 61.0714 1095.91 49.9314 Q1095.91 38.7509 1101.42 32.4315 Q1106.97 26.1121 1116.73 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1172.88 9.54393 L1172.88 15.7418 L1165.75 15.7418 Q1161.74 15.7418 1160.16 17.3622 Q1158.62 18.9825 1158.62 23.1955 L1158.62 27.2059 L1170.89 27.2059 L1170.89 32.9987 L1158.62 32.9987 L1158.62 72.576 L1151.12 72.576 L1151.12 32.9987 L1143.99 32.9987 L1143.99 27.2059 L1151.12 27.2059 L1151.12 24.0462 Q1151.12 16.471 1154.65 13.0277 Q1158.17 9.54393 1165.83 9.54393 L1172.88 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1244.29 48.0275 L1244.29 51.6733 L1210.02 51.6733 Q1210.51 59.3701 1214.64 63.421 Q1218.81 67.4314 1226.23 67.4314 Q1230.52 67.4314 1234.53 66.3781 Q1238.58 65.3249 1242.55 63.2184 L1242.55 70.267 Q1238.54 71.9684 1234.33 72.8596 Q1230.12 73.7508 1225.78 73.7508 Q1214.93 73.7508 1208.57 67.4314 Q1202.25 61.1119 1202.25 50.3365 Q1202.25 39.1965 1208.24 32.6746 Q1214.28 26.1121 1224.49 26.1121 Q1233.64 26.1121 1238.95 32.0264 Q1244.29 37.9003 1244.29 48.0275 M1236.84 45.84 Q1236.76 39.7232 1233.4 36.0774 Q1230.08 32.4315 1224.57 32.4315 Q1218.33 32.4315 1214.56 35.9558 Q1210.83 39.4801 1210.27 45.8805 L1236.84 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1277.15 49.7694 Q1268.11 49.7694 1264.63 51.8354 Q1261.15 53.9013 1261.15 58.8839 Q1261.15 62.8538 1263.74 65.2034 Q1266.37 67.5124 1270.87 67.5124 Q1277.07 67.5124 1280.79 63.1374 Q1284.56 58.7219 1284.56 51.4303 L1284.56 49.7694 L1277.15 49.7694 M1292.01 46.6907 L1292.01 72.576 L1284.56 72.576 L1284.56 65.6895 Q1282.01 69.8214 1278.2 71.8063 Q1274.39 73.7508 1268.88 73.7508 Q1261.92 73.7508 1257.78 69.8619 Q1253.69 65.9325 1253.69 59.3701 Q1253.69 51.7138 1258.8 47.825 Q1263.94 43.9361 1274.11 43.9361 L1284.56 43.9361 L1284.56 43.2069 Q1284.56 38.0623 1281.16 35.2672 Q1277.8 32.4315 1271.68 32.4315 Q1267.79 32.4315 1264.1 33.3632 Q1260.42 34.295 1257.01 36.1584 L1257.01 29.2718 Q1261.11 27.692 1264.95 26.9223 Q1268.8 26.1121 1272.45 26.1121 Q1282.29 26.1121 1287.15 31.2163 Q1292.01 36.3204 1292.01 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1340.02 28.9478 L1340.02 35.9153 Q1336.86 34.1734 1333.66 33.3227 Q1330.5 32.4315 1327.26 32.4315 Q1320.01 32.4315 1316 37.0496 Q1311.99 41.6271 1311.99 49.9314 Q1311.99 58.2358 1316 62.8538 Q1320.01 67.4314 1327.26 67.4314 Q1330.5 67.4314 1333.66 66.5807 Q1336.86 65.6895 1340.02 63.9476 L1340.02 70.8341 Q1336.9 72.2924 1333.54 73.0216 Q1330.21 73.7508 1326.45 73.7508 Q1316.2 73.7508 1310.16 67.3098 Q1304.13 60.8689 1304.13 49.9314 Q1304.13 38.832 1310.2 32.472 Q1316.32 26.1121 1326.93 26.1121 Q1330.38 26.1121 1333.66 26.8413 Q1336.94 27.5299 1340.02 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1390.69 45.1919 L1390.69 72.576 L1383.24 72.576 L1383.24 45.4349 Q1383.24 38.994 1380.73 35.7938 Q1378.22 32.5936 1373.19 32.5936 Q1367.16 32.5936 1363.67 36.4419 Q1360.19 40.2903 1360.19 46.9338 L1360.19 72.576 L1352.7 72.576 L1352.7 9.54393 L1360.19 9.54393 L1360.19 34.2544 Q1362.86 30.163 1366.47 28.1376 Q1370.12 26.1121 1374.86 26.1121 Q1382.67 26.1121 1386.68 30.9732 Q1390.69 35.7938 1390.69 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1439.31 14.324 L1439.31 27.2059 L1454.66 27.2059 L1454.66 32.9987 L1439.31 32.9987 L1439.31 57.6282 Q1439.31 63.1779 1440.8 64.7578 Q1442.34 66.3376 1447 66.3376 L1454.66 66.3376 L1454.66 72.576 L1447 72.576 Q1438.37 72.576 1435.09 69.3758 Q1431.81 66.1351 1431.81 57.6282 L1431.81 32.9987 L1426.34 32.9987 L1426.34 27.2059 L1431.81 27.2059 L1431.81 14.324 L1439.31 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1464.46 27.2059 L1471.91 27.2059 L1471.91 72.576 L1464.46 72.576 L1464.46 27.2059 M1464.46 9.54393 L1471.91 9.54393 L1471.91 18.9825 L1464.46 18.9825 L1464.46 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1494.88 14.324 L1494.88 27.2059 L1510.24 27.2059 L1510.24 32.9987 L1494.88 32.9987 L1494.88 57.6282 Q1494.88 63.1779 1496.38 64.7578 Q1497.92 66.3376 1502.58 66.3376 L1510.24 66.3376 L1510.24 72.576 L1502.58 72.576 Q1493.95 72.576 1490.67 69.3758 Q1487.39 66.1351 1487.39 57.6282 L1487.39 32.9987 L1481.92 32.9987 L1481.92 27.2059 L1487.39 27.2059 L1487.39 14.324 L1494.88 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1520.04 9.54393 L1527.49 9.54393 L1527.49 72.576 L1520.04 72.576 L1520.04 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1581.9 48.0275 L1581.9 51.6733 L1547.63 51.6733 Q1548.11 59.3701 1552.24 63.421 Q1556.42 67.4314 1563.83 67.4314 Q1568.12 67.4314 1572.13 66.3781 Q1576.19 65.3249 1580.15 63.2184 L1580.15 70.267 Q1576.14 71.9684 1571.93 72.8596 Q1567.72 73.7508 1563.38 73.7508 Q1552.53 73.7508 1546.17 67.4314 Q1539.85 61.1119 1539.85 50.3365 Q1539.85 39.1965 1545.84 32.6746 Q1551.88 26.1121 1562.09 26.1121 Q1571.24 26.1121 1576.55 32.0264 Q1581.9 37.9003 1581.9 48.0275 M1574.44 45.84 Q1574.36 39.7232 1571 36.0774 Q1567.68 32.4315 1562.17 32.4315 Q1555.93 32.4315 1552.16 35.9558 Q1548.44 39.4801 1547.87 45.8805 L1574.44 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1620.5 27.2059 L1627.96 27.2059 L1627.96 72.576 L1620.5 72.576 L1620.5 27.2059 M1620.5 9.54393 L1627.96 9.54393 L1627.96 18.9825 L1620.5 18.9825 L1620.5 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1681.27 45.1919 L1681.27 72.576 L1673.81 72.576 L1673.81 45.4349 Q1673.81 38.994 1671.3 35.7938 Q1668.79 32.5936 1663.77 32.5936 Q1657.73 32.5936 1654.25 36.4419 Q1650.76 40.2903 1650.76 46.9338 L1650.76 72.576 L1643.27 72.576 L1643.27 27.2059 L1650.76 27.2059 L1650.76 34.2544 Q1653.44 30.163 1657.04 28.1376 Q1660.69 26.1121 1665.43 26.1121 Q1673.24 26.1121 1677.26 30.9732 Q1681.27 35.7938 1681.27 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1752.36 34.0924 L1752.36 9.54393 L1759.81 9.54393 L1759.81 72.576 L1752.36 72.576 L1752.36 65.7705 Q1750.01 69.8214 1746.4 71.8063 Q1742.84 73.7508 1737.82 73.7508 Q1729.59 73.7508 1724.41 67.1883 Q1719.26 60.6258 1719.26 49.9314 Q1719.26 39.2371 1724.41 32.6746 Q1729.59 26.1121 1737.82 26.1121 Q1742.84 26.1121 1746.4 28.0971 Q1750.01 30.0415 1752.36 34.0924 M1726.96 49.9314 Q1726.96 58.1548 1730.32 62.8538 Q1733.72 67.5124 1739.64 67.5124 Q1745.55 67.5124 1748.96 62.8538 Q1752.36 58.1548 1752.36 49.9314 Q1752.36 41.7081 1748.96 37.0496 Q1745.55 32.3505 1739.64 32.3505 Q1733.72 32.3505 1730.32 37.0496 Q1726.96 41.7081 1726.96 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1795.78 49.7694 Q1786.75 49.7694 1783.27 51.8354 Q1779.78 53.9013 1779.78 58.8839 Q1779.78 62.8538 1782.38 65.2034 Q1785.01 67.5124 1789.51 67.5124 Q1795.7 67.5124 1799.43 63.1374 Q1803.2 58.7219 1803.2 51.4303 L1803.2 49.7694 L1795.78 49.7694 M1810.65 46.6907 L1810.65 72.576 L1803.2 72.576 L1803.2 65.6895 Q1800.65 69.8214 1796.84 71.8063 Q1793.03 73.7508 1787.52 73.7508 Q1780.55 73.7508 1776.42 69.8619 Q1772.33 65.9325 1772.33 59.3701 Q1772.33 51.7138 1777.43 47.825 Q1782.58 43.9361 1792.75 43.9361 L1803.2 43.9361 L1803.2 43.2069 Q1803.2 38.0623 1799.8 35.2672 Q1796.43 32.4315 1790.32 32.4315 Q1786.43 32.4315 1782.74 33.3632 Q1779.05 34.295 1775.65 36.1584 L1775.65 29.2718 Q1779.74 27.692 1783.59 26.9223 Q1787.44 26.1121 1791.09 26.1121 Q1800.93 26.1121 1805.79 31.2163 Q1810.65 36.3204 1810.65 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1833.38 14.324 L1833.38 27.2059 L1848.73 27.2059 L1848.73 32.9987 L1833.38 32.9987 L1833.38 57.6282 Q1833.38 63.1779 1834.88 64.7578 Q1836.42 66.3376 1841.07 66.3376 L1848.73 66.3376 L1848.73 72.576 L1841.07 72.576 Q1832.45 72.576 1829.16 69.3758 Q1825.88 66.1351 1825.88 57.6282 L1825.88 32.9987 L1820.41 32.9987 L1820.41 27.2059 L1825.88 27.2059 L1825.88 14.324 L1833.38 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1879.15 49.7694 Q1870.12 49.7694 1866.63 51.8354 Q1863.15 53.9013 1863.15 58.8839 Q1863.15 62.8538 1865.74 65.2034 Q1868.38 67.5124 1872.87 67.5124 Q1879.07 67.5124 1882.8 63.1374 Q1886.57 58.7219 1886.57 51.4303 L1886.57 49.7694 L1879.15 49.7694 M1894.02 46.6907 L1894.02 72.576 L1886.57 72.576 L1886.57 65.6895 Q1884.01 69.8214 1880.21 71.8063 Q1876.4 73.7508 1870.89 73.7508 Q1863.92 73.7508 1859.79 69.8619 Q1855.7 65.9325 1855.7 59.3701 Q1855.7 51.7138 1860.8 47.825 Q1865.95 43.9361 1876.11 43.9361 L1886.57 43.9361 L1886.57 43.2069 Q1886.57 38.0623 1883.16 35.2672 Q1879.8 32.4315 1873.68 32.4315 Q1869.79 32.4315 1866.11 33.3632 Q1862.42 34.295 1859.02 36.1584 L1859.02 29.2718 Q1863.11 27.692 1866.96 26.9223 Q1870.81 26.1121 1874.45 26.1121 Q1884.3 26.1121 1889.16 31.2163 Q1894.02 36.3204 1894.02 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1938.3 28.5427 L1938.3 35.5912 Q1935.14 33.9709 1931.73 33.1607 Q1928.33 32.3505 1924.68 32.3505 Q1919.13 32.3505 1916.34 34.0519 Q1913.58 35.7533 1913.58 39.156 Q1913.58 41.7486 1915.57 43.2475 Q1917.55 44.7058 1923.55 46.0426 L1926.1 46.6097 Q1934.04 48.3111 1937.36 51.4303 Q1940.73 54.509 1940.73 60.0587 Q1940.73 66.3781 1935.7 70.0644 Q1930.72 73.7508 1921.97 73.7508 Q1918.32 73.7508 1914.35 73.0216 Q1910.43 72.3329 1906.05 70.9151 L1906.05 63.2184 Q1910.18 65.3654 1914.19 66.4591 Q1918.2 67.5124 1922.13 67.5124 Q1927.4 67.5124 1930.23 65.73 Q1933.07 63.9071 1933.07 60.6258 Q1933.07 57.5877 1931 55.9673 Q1928.98 54.3469 1922.05 52.8481 L1919.46 52.2405 Q1912.53 50.7821 1909.45 47.7845 Q1906.37 44.7463 1906.37 39.4801 Q1906.37 33.0797 1910.91 29.5959 Q1915.45 26.1121 1923.79 26.1121 Q1927.93 26.1121 1931.57 26.7198 Q1935.22 27.3274 1938.3 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M1991.4 48.0275 L1991.4 51.6733 L1957.13 51.6733 Q1957.62 59.3701 1961.75 63.421 Q1965.92 67.4314 1973.34 67.4314 Q1977.63 67.4314 1981.64 66.3781 Q1985.69 65.3249 1989.66 63.2184 L1989.66 70.267 Q1985.65 71.9684 1981.44 72.8596 Q1977.22 73.7508 1972.89 73.7508 Q1962.03 73.7508 1955.67 67.4314 Q1949.35 61.1119 1949.35 50.3365 Q1949.35 39.1965 1955.35 32.6746 Q1961.39 26.1121 1971.59 26.1121 Q1980.75 26.1121 1986.06 32.0264 Q1991.4 37.9003 1991.4 48.0275 M1983.95 45.84 Q1983.87 39.7232 1980.51 36.0774 Q1977.18 32.4315 1971.67 32.4315 Q1965.44 32.4315 1961.67 35.9558 Q1957.94 39.4801 1957.38 45.8805 L1983.95 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip170)\" d=\"M 0 0 M2011.01 14.324 L2011.01 27.2059 L2026.36 27.2059 L2026.36 32.9987 L2011.01 32.9987 L2011.01 57.6282 Q2011.01 63.1779 2012.51 64.7578 Q2014.05 66.3376 2018.71 66.3376 L2026.36 66.3376 L2026.36 72.576 L2018.71 72.576 Q2010.08 72.576 2006.8 69.3758 Q2003.51 66.1351 2003.51 57.6282 L2003.51 32.9987 L1998.05 32.9987 L1998.05 27.2059 L2003.51 27.2059 L2003.51 14.324 L2011.01 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip172)\" d=\"\n",
       "M351.748 160.256 L351.748 1386.4 L453.615 1386.4 L453.615 160.256 L351.748 160.256 L351.748 160.256  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  351.748,160.256 351.748,1386.4 453.615,1386.4 453.615,160.256 351.748,160.256 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M479.082 1081.04 L479.082 1386.4 L580.949 1386.4 L580.949 1081.04 L479.082 1081.04 L479.082 1081.04  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  479.082,1081.04 479.082,1386.4 580.949,1386.4 580.949,1081.04 479.082,1081.04 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M606.416 960.324 L606.416 1386.4 L708.283 1386.4 L708.283 960.324 L606.416 960.324 L606.416 960.324  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  606.416,960.324 606.416,1386.4 708.283,1386.4 708.283,960.324 606.416,960.324 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M733.75 1291.71 L733.75 1386.4 L835.617 1386.4 L835.617 1291.71 L733.75 1291.71 L733.75 1291.71  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  733.75,1291.71 733.75,1386.4 835.617,1386.4 835.617,1291.71 733.75,1291.71 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M861.084 1384.03 L861.084 1386.4 L962.951 1386.4 L962.951 1384.03 L861.084 1384.03 L861.084 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  861.084,1384.03 861.084,1386.4 962.951,1386.4 962.951,1384.03 861.084,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M988.418 1372.19 L988.418 1386.4 L1090.28 1386.4 L1090.28 1372.19 L988.418 1372.19 L988.418 1372.19  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  988.418,1372.19 988.418,1386.4 1090.28,1386.4 1090.28,1372.19 988.418,1372.19 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1115.75 1369.83 L1115.75 1386.4 L1217.62 1386.4 L1217.62 1369.83 L1115.75 1369.83 L1115.75 1369.83  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1115.75,1369.83 1115.75,1386.4 1217.62,1386.4 1217.62,1369.83 1115.75,1369.83 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1243.09 1384.03 L1243.09 1386.4 L1344.95 1386.4 L1344.95 1384.03 L1243.09 1384.03 L1243.09 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1243.09,1384.03 1243.09,1386.4 1344.95,1386.4 1344.95,1384.03 1243.09,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1370.42 1384.03 L1370.42 1386.4 L1472.29 1386.4 L1472.29 1384.03 L1370.42 1384.03 L1370.42 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1370.42,1384.03 1370.42,1386.4 1472.29,1386.4 1472.29,1384.03 1370.42,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1497.75 1381.66 L1497.75 1386.4 L1599.62 1386.4 L1599.62 1381.66 L1497.75 1381.66 L1497.75 1381.66  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1497.75,1381.66 1497.75,1386.4 1599.62,1386.4 1599.62,1381.66 1497.75,1381.66 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1625.09 1384.03 L1625.09 1386.4 L1726.96 1386.4 L1726.96 1384.03 L1625.09 1384.03 L1625.09 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1625.09,1384.03 1625.09,1386.4 1726.96,1386.4 1726.96,1384.03 1625.09,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1752.42 1381.66 L1752.42 1386.4 L1854.29 1386.4 L1854.29 1381.66 L1752.42 1381.66 L1752.42 1381.66  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1752.42,1381.66 1752.42,1386.4 1854.29,1386.4 1854.29,1381.66 1752.42,1381.66 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M1879.76 1384.03 L1879.76 1386.4 L1981.62 1386.4 L1981.62 1384.03 L1879.76 1384.03 L1879.76 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1879.76,1384.03 1879.76,1386.4 1981.62,1386.4 1981.62,1384.03 1879.76,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M2007.09 1384.03 L2007.09 1386.4 L2108.96 1386.4 L2108.96 1384.03 L2007.09 1384.03 L2007.09 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.09,1384.03 2007.09,1386.4 2108.96,1386.4 2108.96,1384.03 2007.09,1384.03 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip172)\" d=\"\n",
       "M2134.42 1384.03 L2134.42 1386.4 L2236.29 1386.4 L2236.29 1384.03 L2134.42 1384.03 L2134.42 1384.03  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip172)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2134.42,1384.03 2134.42,1386.4 2236.29,1386.4 2236.29,1384.03 2134.42,1384.03 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar(titles, freq_array, \n",
    "    title=\"Occurencies of each title in dataset\", \n",
    "    xlabel=\"Titles\",\n",
    "    ylabel=\"Number of occurences\",\n",
    "    legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode titles\n",
    "Mr -> 0, Mrs-> 1 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_to_numerical (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for encoding titles\n",
    "# replaces titles with its index in the titles array\n",
    "function title_to_numerical(value)\n",
    "    return findfirst(isequal(value), titles_list) - 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Int64}:\n",
       " 1\n",
       " 0\n",
       " 3\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 5\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Title = map(x -> title_to_numerical(x), data.Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put titles in categories based on frequency\n",
    "Mr(encoded as 0) -> 0,\n",
    "Mrs(1) -> 1,\n",
    "Miss(2) -> 2,\n",
    "rest -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorize_numerical_title (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function categorize_numerical_title(value)\n",
    "   if value > 2\n",
    "        return 3\n",
    "    else\n",
    "        return value\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Int64}:\n",
       " 1\n",
       " 0\n",
       " 3\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Title = map(x -> categorize_numerical_title(x), data.Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in age column\n",
    "Replace missing values with random values according to the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Float64}:\n",
       " 22.0\n",
       " 38.0\n",
       " 26.0\n",
       " 35.0\n",
       " 35.0\n",
       " 27.0\n",
       " 54.0\n",
       "  2.0\n",
       " 27.0\n",
       " 14.0\n",
       "  4.0\n",
       " 58.0\n",
       " 20.0\n",
       "  ⋮\n",
       " 56.0\n",
       " 25.0\n",
       " 33.0\n",
       " 22.0\n",
       " 28.0\n",
       " 25.0\n",
       " 39.0\n",
       " 27.0\n",
       " 19.0\n",
       " 39.0\n",
       " 26.0\n",
       " 32.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = mean(skipmissing(data.Age))\n",
    "std_age = std(skipmissing(data.Age))\n",
    "data.Age = map(x -> ismissing(x)\n",
    "                    # if value is missing replace it with a random int\n",
    "                    # in range [mean - sample-deviation, mean + sample-deviation]\n",
    "                    ? rand(floor(mean_age - std_age) : floor(mean_age + std_age))\n",
    "                    : x,\n",
    "                data.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put ages in categories based on frequency\n",
    "or survivability ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats:\n",
      "Length:         891\n",
      "Missing Count:  0\n",
      "Mean:           29.581560\n",
      "Minimum:        0.420000\n",
      "1st Quartile:   21.000000\n",
      "Median:         28.000000\n",
      "3rd Quartile:   38.000000\n",
      "Maximum:        80.000000\n",
      "Type:           Float64\n"
     ]
    }
   ],
   "source": [
    "describe(data.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorize_age (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function categorize_age(value)\n",
    "    if value < 16\n",
    "        return 0\n",
    "    elseif value < 30\n",
    "        return 1\n",
    "    elseif value < 50\n",
    "        return 2\n",
    "    else return 3 end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 3\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 1\n",
       " ⋮\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Age = map(x -> categorize_age(x), data.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify sibSp and parch into a single column (familyOnBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 4\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 5\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Family = map(x -> x.SibSp + x.Parch, eachrow(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize family size\n",
    "0 -> 0,\n",
    "1 -> 1,\n",
    "2 -> 2,\n",
    "rest -> 3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 0\n",
       " 0\n",
       " 3\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Family = map(x -> x > 2 ? 3 : x, data.Family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode embarkation port\n",
    "S -> 0, \n",
    "C -> 1, \n",
    "Q -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_port (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode_port(value)\n",
    "   # replace missing values with 0 since \"S\" is the most embarked port\n",
    "   if ismissing(value) || value == \"S\"\n",
    "        return 0\n",
    "    elseif value == \"C\"\n",
    "        return 1\n",
    "    else return 2\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891-element PooledArrays.PooledVector{Int64, UInt32, Vector{UInt32}}:\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Embarked = map(x -> encode_port(x), data.Embarked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: find a way to sue the other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unused data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>7 rows × 7 columns</p><tr><th>1</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Title</td><td>1.35802</td><td>0</td><td>1.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>Sex</td><td>0.352413</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>5</th><td>Age</td><td>1.44669</td><td>0</td><td>1.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>6</th><td>Embarked</td><td>0.361392</td><td>0</td><td>0.0</td><td>2</td><td>0</td><td>Int64</td></tr><tr><th>7</th><td>Family</td><td>0.716049</td><td>0</td><td>0.0</td><td>3</td><td>0</td><td>Int64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Int64 & Float64 & Int64 & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Survived & 0.383838 & 0 & 0.0 & 1 & 0 & Int64 \\\\\n",
       "\t2 & Pclass & 2.30864 & 1 & 3.0 & 3 & 0 & Int64 \\\\\n",
       "\t3 & Title & 1.35802 & 0 & 1.0 & 3 & 0 & Int64 \\\\\n",
       "\t4 & Sex & 0.352413 & 0 & 0.0 & 1 & 0 & Int64 \\\\\n",
       "\t5 & Age & 1.44669 & 0 & 1.0 & 3 & 0 & Int64 \\\\\n",
       "\t6 & Embarked & 0.361392 & 0 & 0.0 & 2 & 0 & Int64 \\\\\n",
       "\t7 & Family & 0.716049 & 0 & 0.0 & 3 & 0 & Int64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m7×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean     \u001b[0m\u001b[1m min   \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max   \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │ Survived  0.383838      0      0.0      1         0  Int64\n",
       "   2 │ Pclass    2.30864       1      3.0      3         0  Int64\n",
       "   3 │ Title     1.35802       0      1.0      3         0  Int64\n",
       "   4 │ Sex       0.352413      0      0.0      1         0  Int64\n",
       "   5 │ Age       1.44669       0      1.0      3         0  Int64\n",
       "   6 │ Embarked  0.361392      0      0.0      2         0  Int64\n",
       "   7 │ Family    0.716049      0      0.0      3         0  Int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unused_cols = [\"Ticket\", \"Fare\", \"Cabin\", \"PassengerId\", \"SibSp\", \"Parch\"]\n",
    "select!(data, Not(unused_cols))\n",
    "\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine correlation of each column with the \"survived\" property "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived - Pclass correlation value: -0.33848103596101586\n",
      "Survived - Title correlation value: 0.16003889129843982\n",
      "Survived - Sex correlation value: 0.5433513806577527\n",
      "Survived - Age correlation value: -0.05815923939190801\n",
      "Survived - Embarked correlation value: 0.10681138570892197\n",
      "Survived - Family correlation value: 0.12441321782689874\n"
     ]
    }
   ],
   "source": [
    "cols = [\"Pclass\", \"Title\", \"Sex\", \"Age\", \"Embarked\", \"Family\"]\n",
    "for value in cols\n",
    "    corr = cor(data.Survived, data[:, \"$value\"])\n",
    "    println(\"Survived - $value correlation value: $corr\") \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬─────────┬────────────┐\n",
       "│\u001b[22m _.names  \u001b[0m│\u001b[22m _.types \u001b[0m│\u001b[22m _.scitypes \u001b[0m│\n",
       "├──────────┼─────────┼────────────┤\n",
       "│ Survived │ Int64   │ Count      │\n",
       "│ Pclass   │ Int64   │ Count      │\n",
       "│ Title    │ Int64   │ Count      │\n",
       "│ Sex      │ Int64   │ Count      │\n",
       "│ Age      │ Int64   │ Count      │\n",
       "│ Embarked │ Int64   │ Count      │\n",
       "│ Family   │ Int64   │ Count      │\n",
       "└──────────┴─────────┴────────────┘\n",
       "_.nrows = 891\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must change the data type of the Survived class to a Finite one (OrderedFactor or Multiclass) and the others to Continous in order to fit some classifying algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(data, :Survived => OrderedFactor)\n",
    "coerce!(data, Count => Continuous);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬─────────────────────────────────┬──────────────────┐\n",
       "│\u001b[22m _.names  \u001b[0m│\u001b[22m _.types                         \u001b[0m│\u001b[22m _.scitypes       \u001b[0m│\n",
       "├──────────┼─────────────────────────────────┼──────────────────┤\n",
       "│ Survived │ CategoricalValue{Int64, UInt32} │ OrderedFactor{2} │\n",
       "│ Pclass   │ Float64                         │ Continuous       │\n",
       "│ Title    │ Float64                         │ Continuous       │\n",
       "│ Sex      │ Float64                         │ Continuous       │\n",
       "│ Age      │ Float64                         │ Continuous       │\n",
       "│ Embarked │ Float64                         │ Continuous       │\n",
       "│ Family   │ Float64                         │ Continuous       │\n",
       "└──────────┴─────────────────────────────────┴──────────────────┘\n",
       "_.nrows = 891\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data now is in the right format.\n",
    "The next step is to break it into examples and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 0, 0, 0, 1, 0, 0, 1, 1, 1  …  1, 0, 0, 0, 0, 1, 0, 0, 1, 0], \u001b[1m891×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Pclass  \u001b[0m\u001b[1m Title   \u001b[0m\u001b[1m Sex     \u001b[0m\u001b[1m Age     \u001b[0m\u001b[1m Embarked \u001b[0m\u001b[1m Family  \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────\n",
       "   1 │     3.0      0.0      1.0      2.0       0.0      0.0\n",
       "   2 │     3.0      2.0      0.0      0.0       2.0      3.0\n",
       "   3 │     3.0      1.0      0.0      1.0       0.0      0.0\n",
       "   4 │     2.0      1.0      0.0      2.0       0.0      1.0\n",
       "   5 │     1.0      3.0      1.0      3.0       1.0      1.0\n",
       "   6 │     3.0      1.0      0.0      2.0       0.0      0.0\n",
       "   7 │     3.0      1.0      0.0      2.0       0.0      0.0\n",
       "   8 │     3.0      1.0      0.0      2.0       0.0      0.0\n",
       "   9 │     3.0      0.0      1.0      1.0       1.0      2.0\n",
       "  10 │     2.0      0.0      1.0      1.0       0.0      2.0\n",
       "  11 │     2.0      1.0      0.0      1.0       0.0      0.0\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n",
       " 882 │     1.0      1.0      0.0      1.0       1.0      0.0\n",
       " 883 │     3.0      1.0      0.0      1.0       1.0      1.0\n",
       " 884 │     3.0      1.0      0.0      1.0       0.0      1.0\n",
       " 885 │     3.0      1.0      0.0      1.0       1.0      0.0\n",
       " 886 │     3.0      1.0      0.0      1.0       0.0      0.0\n",
       " 887 │     1.0      3.0      1.0      0.0       0.0      1.0\n",
       " 888 │     2.0      1.0      0.0      1.0       0.0      0.0\n",
       " 889 │     3.0      1.0      0.0      1.0       0.0      0.0\n",
       " 890 │     3.0      1.0      0.0      2.0       0.0      0.0\n",
       " 891 │     3.0      1.0      0.0      2.0       2.0      0.0\n",
       "\u001b[36m                                             870 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:Survived), x->true, rng=123)\n",
    "# shuffle data with a seed to mentain a certain consistency between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  615, 616, 617, 618, 619, 620, 621, 622, 623, 624], [625, 626, 627, 628, 629, 630, 631, 632, 633, 634  …  882, 883, 884, 885, 886, 887, 888, 889, 890, 891])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = partition(eachindex(y), 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the same transformations to the data to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = CSV.File(\"data/test.csv\") |> DataFrame\n",
    "x_pred.Sex = map(x -> x == \"male\" ? 0 : 1, x_pred.Sex)\n",
    "rename!(x_pred, :Name => :Title)\n",
    "x_pred.Title = map(x -> substring_in_string(x, titles_list), x_pred.Title)\n",
    "x_pred.Title = map(x -> title_to_numerical(x), x_pred.Title);\n",
    "x_pred.Title = map(x -> categorize_numerical_title(x), x_pred.Title)\n",
    "x_pred.Age = map(x -> ismissing(x)\n",
    "                    # if value is missing replace it with a random int\n",
    "                    # in range [mean - sample-deviation, mean + sample-deviation]\n",
    "                    ? rand(floor(mean_age - std_age) : floor(mean_age + std_age))\n",
    "                    : x,\n",
    "                x_pred.Age)\n",
    "x_pred.Age = map(x -> categorize_age(x), x_pred.Age)\n",
    "x_pred.Family = map(x -> x.SibSp + x.Parch, eachrow(x_pred))\n",
    "x_pred.Family = map(x -> x > 2 ? 3 : x, x_pred.Family);\n",
    "x_pred.Embarked = map(x -> encode_port(x), x_pred.Embarked);\n",
    "select!(x_pred, Not(unused_cols));\n",
    "coerce!(x_pred, Count => Continuous);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find models that can be fitted to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype), T} where T<:Tuple}:\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X, y))[16:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what results we get with a DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[35mCART decision tree classifier.\u001b[39m\n",
       "\u001b[35m→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).\u001b[39m\n",
       "\u001b[35m→ do `@load DecisionTreeClassifier pkg=\"DecisionTree\"` to use the model.\u001b[39m\n",
       "\u001b[35m→ do `?DecisionTreeClassifier` for documentation.\u001b[39m\n",
       "(name = \"DecisionTreeClassifier\",\n",
       " package_name = \"DecisionTree\",\n",
       " is_supervised = true,\n",
       " docstring = \"CART decision tree classifier.\\n→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).\\n→ do `@load DecisionTreeClassifier pkg=\\\"DecisionTree\\\"` to use the model.\\n→ do `?DecisionTreeClassifier` for documentation.\",\n",
       " hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n",
       " hyperparameter_types = (\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Float64\", \"Float64\", \"Int64\"),\n",
       " hyperparameters = (:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold, :pdf_smoothing, :display_depth),\n",
       " implemented_methods = [:predict, :clean!, :fit, :fitted_params],\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " load_path = \"MLJDecisionTreeInterface.DecisionTreeClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/bensadeghi/DecisionTree.jl\",\n",
       " package_uuid = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\",\n",
       " prediction_type = :probabilistic,\n",
       " supports_online = false,\n",
       " supports_weights = false,\n",
       " input_scitype = Table{_s24} where _s24<:Union{AbstractVector{_s23} where _s23<:Continuous, AbstractVector{_s23} where _s23<:Count, AbstractVector{_s23} where _s23<:OrderedFactor},\n",
       " target_scitype = AbstractVector{_s33} where _s33<:Finite,\n",
       " output_scitype = Unknown,)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(\"DecisionTreeClassifier\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = @load DecisionTreeClassifier pkg=\"DecisionTree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @296\u001b[39m"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier,…} @018\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @511\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @134\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 3, Threshold 0.5\n",
      "L-> Feature 1, Threshold 1.5\n",
      "    L-> Feature 4, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 6, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "            R-> 1 : 4/4\n",
      "        R-> Feature 6, Threshold 0.5\n",
      "            L-> Feature 5, Threshold 0.5\n",
      "                L-> \n",
      "                R-> 2 : 1/1\n",
      "            R-> Feature 5, Threshold 0.5\n",
      "                L-> 1 : 4/4\n",
      "                R-> \n",
      "    R-> Feature 2, Threshold 1.5\n",
      "        L-> Feature 4, Threshold 2.5\n",
      "            L-> Feature 6, Threshold 2.5\n",
      "                L-> \n",
      "                R-> 1 : 12/12\n",
      "            R-> 1 : 13/13\n",
      "        R-> Feature 6, Threshold 2.5\n",
      "            L-> Feature 2, Threshold 2.5\n",
      "                L-> 2 : 9/9\n",
      "                R-> 1 : 4/4\n",
      "            R-> Feature 1, Threshold 2.5\n",
      "                L-> 2 : 1/1\n",
      "                R-> \n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 4, Threshold 1.5\n",
      "        L-> Feature 5, Threshold 0.5\n",
      "            L-> Feature 2, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "            R-> 2 : 16/16\n",
      "        R-> Feature 4, Threshold 2.5\n",
      "            L-> 2 : 49/49\n",
      "            R-> Feature 6, Threshold 0.5\n",
      "                L-> \n",
      "                R-> 2 : 12/12\n",
      "    R-> Feature 5, Threshold 0.5\n",
      "        L-> Feature 6, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "        R-> Feature 4, Threshold 0.5\n",
      "            L-> Feature 6, Threshold 1.5\n",
      "                L-> 2 : 5/5\n",
      "                R-> \n",
      "            R-> Feature 5, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier,…} @018\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier,…} @018\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @511\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @134\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element MLJBase.UnivariateFiniteVector{Multiclass{2}, Int64, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.75, 1=>0.25)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.923, 1=>0.0769)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.857, 1=>0.143)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>1.0, 1=>0.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.923, 1=>0.0769)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.364, 1=>0.636)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.847, 1=>0.153)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.899, 1=>0.101)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.6, 1=>0.4)\n",
       " UnivariateFinite{Multiclass{2}}(0=>1.0, 1=>0.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.75, 1=>0.25)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.6, 1=>0.4)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.899, 1=>0.101)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.0, 1=>1.0)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.913, 1=>0.087)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.899, 1=>0.101)\n",
       " UnivariateFinite{Multiclass{2}}(0=>0.847, 1=>0.153)\n",
       " UnivariateFinite{Multiclass{2}}(0=>1.0, 1=>0.0)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, X[test, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1947565543071161"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results are not bad for the \"vanilla\" decision tree. \n",
    "Kaggle score: 0.76076\n",
    "\n",
    "In the next steps, we will optimize the hyper parameters with a grid search and evaluate the performance on the test set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(model, :max_depth, lower=-1, upper=5)\n",
    "r2 = range(model, :min_samples_leaf, lower=1, upper=20)\n",
    "r3 = range(model, :min_samples_split, lower=1, upper=40)\n",
    "r4 = range(model, :n_subfeatures, lower=0, upper=5)\n",
    "r5 = range(model, :post_prune, values=[false, true])\n",
    "r6 = range(model, :merge_purity_threshold, lower=0.85, upper=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = TunedModel(model=model,\n",
    "                         ranges=[r, r2, r3, r4, r5, r6],\n",
    "                         resampling=CV(nfolds=6),\n",
    "                         measures=cross_entropy,\n",
    "#                          acceleration=CPUThreads(), # this might be faster, depending on your OS/machine\n",
    "                         acceleration=CPUProcesses(), \n",
    "                         n = 50000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @822\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 50000 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 50000 metamodels: 100%[=========================] Time: 0:03:34\u001b[39m2:40\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @822\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @436\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @613\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_mach = machine(tuned_model, X, y)\n",
    "fit!(tuned_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = 5,\n",
       "    min_samples_leaf = 12,\n",
       "    min_samples_split = 31,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 4,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.8666666666666667,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @621\u001b[39m"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = report(tuned_mach);\n",
    "rep.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = predict(tuned_mach, X[test, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16104868913857678"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slight performance increase but we take those\n",
    "kaggle score: 0.77751 training on 0.7% data\n",
    "\n",
    "kaggle score: 0.76315 with all data :c -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: try regularization or encoding before applying algortihm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks time\n",
    "kaggle score: 0.77751\n",
    "\n",
    "Let's first normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standardizer(\n",
       "    features = Symbol[],\n",
       "    ignore = false,\n",
       "    ordered_factor = false,\n",
       "    count = false)\u001b[34m @702\u001b[39m"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand = Standardizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJFlux.NeuralNetworkClassifier"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = @load NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "    builder = Short(\n",
       "            n_hidden = 20,\n",
       "            dropout = 0.2,\n",
       "            σ = NNlib.elu),\n",
       "    finaliser = NNlib.softmax,\n",
       "    optimiser = ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n",
       "    loss = Flux.Losses.crossentropy,\n",
       "    epochs = 200,\n",
       "    batch_size = 100,\n",
       "    lambda = 0.0,\n",
       "    alpha = 0.0,\n",
       "    optimiser_changes_trigger_retraining = false,\n",
       "    acceleration = CPU1{Nothing}(nothing))\u001b[34m @969\u001b[39m"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(builder=MLJFlux.Short(n_hidden=20, σ=Flux.elu, dropout=0.2), \n",
    "     epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline442(\n",
       "    standardizer = Standardizer(\n",
       "            features = Symbol[],\n",
       "            ignore = false,\n",
       "            ordered_factor = false,\n",
       "            count = false),\n",
       "    neural_network_classifier = NeuralNetworkClassifier(\n",
       "            builder = \u001b[34mShort @999\u001b[39m,\n",
       "            finaliser = NNlib.softmax,\n",
       "            optimiser = ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n",
       "            loss = Flux.Losses.crossentropy,\n",
       "            epochs = 200,\n",
       "            batch_size = 100,\n",
       "            lambda = 0.0,\n",
       "            alpha = 0.0,\n",
       "            optimiser_changes_trigger_retraining = false,\n",
       "            acceleration = CPU1{Nothing}(nothing)))\u001b[34m @301\u001b[39m"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline that first standardizes data then runs it on the NN\n",
    "pipe = @pipeline stand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NumericRange)(Float64, :(neural_network_classifier.builder.dropout), ... )"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(pipe, :(neural_network_classifier.lambda), lower=0.0, upper=10.0)\n",
    "r2 = range(pipe, :(neural_network_classifier.builder.dropout), lower=0, upper=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedModel(model=pipe,\n",
    "                   ranges = [r, r2],\n",
    "                   resampling=Holdout(fraction_train=0.7, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   repeats=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @832\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @974\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @084\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @832\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 100 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:02:33\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @832\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @974\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @084\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(mach, X[test, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1797752808988764"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "    builder = Short(\n",
       "            n_hidden = 20,\n",
       "            dropout = 0.0,\n",
       "            σ = NNlib.elu),\n",
       "    finaliser = NNlib.softmax,\n",
       "    optimiser = ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n",
       "    loss = Flux.Losses.crossentropy,\n",
       "    epochs = 200,\n",
       "    batch_size = 100,\n",
       "    lambda = 4.444444444444445,\n",
       "    alpha = 0.0,\n",
       "    optimiser_changes_trigger_retraining = false,\n",
       "    acceleration = CPU1{Nothing}(nothing))\u001b[34m @438\u001b[39m"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach).best_model.neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mode.(MLJ.predict(mach, x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(predictions, \"data/NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"NN777.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier\n",
    "keggel score: 0.77990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[35mK-Nearest Neighbors classifier: predicts the class associated with a new point\u001b[39m\n",
       "\u001b[35mby taking a vote over the classes of the K-nearest points.\u001b[39m\n",
       "\n",
       "\u001b[35m→ based on [NearestNeighborModels](https://github.com/alan-turing-institute/NearestNeighborModels.jl).\u001b[39m\n",
       "\u001b[35m→ do `@load KNNClassifier pkg=\"NearestNeighborModels\"` to use the model.\u001b[39m\n",
       "\u001b[35m→ do `?KNNClassifier` for documentation.\u001b[39m\n",
       "(name = \"KNNClassifier\",\n",
       " package_name = \"NearestNeighborModels\",\n",
       " is_supervised = true,\n",
       " docstring = \"K-Nearest Neighbors classifier: predicts the class associated with a new point\\nby taking a vote over the classes of the K-nearest points.\\n\\n→ based on [NearestNeighborModels](https://github.com/alan-turing-institute/NearestNeighborModels.jl).\\n→ do `@load KNNClassifier pkg=\\\"NearestNeighborModels\\\"` to use the model.\\n→ do `?KNNClassifier` for documentation.\",\n",
       " hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing),\n",
       " hyperparameter_types = (\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\"),\n",
       " hyperparameters = (:K, :algorithm, :metric, :leafsize, :reorder, :weights),\n",
       " implemented_methods = [:predict, :clean!, :fit, :fitted_params],\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " load_path = \"NearestNeighborModels.KNNClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/alan-turing-institute/NearestNeighborModels.jl\",\n",
       " package_uuid = \"6f286f6a-111f-5878-ab1e-185364afe411\",\n",
       " prediction_type = :probabilistic,\n",
       " supports_online = false,\n",
       " supports_weights = true,\n",
       " input_scitype = Table{_s24} where _s24<:(AbstractVector{_s23} where _s23<:Continuous),\n",
       " target_scitype = AbstractVector{_s45} where _s45<:Finite,\n",
       " output_scitype = Unknown,)"
      ]
     },
     "execution_count": 1165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(\"KNN\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import NearestNeighborModels ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNNClassifier"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KNNClassifier pkg=\"NearestNeighborModels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = Uniform())\u001b[34m @661\u001b[39m"
      ]
     },
     "execution_count": 1170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline513(\n",
       "    standardizer = Standardizer(\n",
       "            features = Symbol[],\n",
       "            ignore = false,\n",
       "            ordered_factor = false,\n",
       "            count = false),\n",
       "    knn_classifier = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = Uniform()))\u001b[34m @141\u001b[39m"
      ]
     },
     "execution_count": 1171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = @pipeline stand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NumericRange)(Int64, :(knn_classifier.K), ... )"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(pipe, :(knn_classifier.K), lower=2, upper=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = Pipeline513(\n",
       "            standardizer = \u001b[34mStandardizer @354\u001b[39m,\n",
       "            knn_classifier = \u001b[34mKNNClassifier @661\u001b[39m),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = Holdout(\n",
       "            fraction_train = 0.8,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = LogLoss(\n",
       "            tol = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = NumericRange(\n",
       "            field = :(knn_classifier.K),\n",
       "            lower = 2,\n",
       "            upper = 25,\n",
       "            origin = 13.5,\n",
       "            unit = 11.5,\n",
       "            scale = :linear),\n",
       "    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "    train_best = true,\n",
       "    repeats = 5,\n",
       "    n = nothing,\n",
       "    acceleration = CPU1{Nothing}(nothing),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true,\n",
       "    cache = true)\u001b[34m @278\u001b[39m"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TunedModel(model=pipe,\n",
    "                   ranges=r,\n",
    "                   resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   repeats=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @066\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @231\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @110\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @066\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 10 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 10 metamodels: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @066\u001b[39m trained 3 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @231\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @110\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 1219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 1220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(predict(mach, X[test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18726591760299627"
      ]
     },
     "execution_count": 1223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline513(\n",
       "    standardizer = Standardizer(\n",
       "            features = Symbol[],\n",
       "            ignore = false,\n",
       "            ordered_factor = false,\n",
       "            count = false),\n",
       "    knn_classifier = KNNClassifier(\n",
       "            K = 20,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = Uniform()))\u001b[34m @524\u001b[39m"
      ]
     },
     "execution_count": 1213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach).best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticClassifier\n",
    "kaggle score: 0.75598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some correlation between the columns in the data.\n",
    "\n",
    "To get rid of it, we will first apply PCA before the actual logistic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJMultivariateStatsInterface ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(\n",
       "    maxoutdim = 0,\n",
       "    method = :auto,\n",
       "    pratio = 0.99,\n",
       "    mean = nothing)\u001b[34m @710\u001b[39m"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLinearModels ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticClassifier"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load LogisticClassifier pkg=\"MLJLinearModels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticClassifier(\n",
       "    lambda = 1.0,\n",
       "    gamma = 0.0,\n",
       "    penalty = :l2,\n",
       "    fit_intercept = true,\n",
       "    penalize_intercept = false,\n",
       "    solver = nothing)\u001b[34m @448\u001b[39m"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(model, :penalty, values=[:l1, :l2, :en])\n",
    "r2 = range(model, :lambda, lower=0.1, upper=10.0)\n",
    "r3 = range(model, :gamma, lower=0.0, upper=10.0)\n",
    "r4 = range(model, :fit_intercept, values=[false, true])\n",
    "r5 = range(model, :penalize_intercept, values=[false, true]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedModel(model=model,\n",
    "                  ranges=[r, r2, r3, r4, r5],\n",
    "                  resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                  measures=cross_entropy,\n",
    "                  repeats=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline441(\n",
       "    pca = PCA(\n",
       "            maxoutdim = 0,\n",
       "            method = :auto,\n",
       "            pratio = 0.99,\n",
       "            mean = nothing),\n",
       "    probabilistic_tuned_model = ProbabilisticTunedModel(\n",
       "            model = \u001b[34mLogisticClassifier @448\u001b[39m,\n",
       "            tuning = \u001b[34mGrid @056\u001b[39m,\n",
       "            resampling = \u001b[34mHoldout @541\u001b[39m,\n",
       "            measure = \u001b[34mLogLoss{Float64} @120\u001b[39m,\n",
       "            weights = nothing,\n",
       "            operation = MLJModelInterface.predict,\n",
       "            range = MLJBase.ParamRange[\u001b[34mNominalRange{Symbol,…} @641\u001b[39m, \u001b[34mNumericRange{Float64,…} @484\u001b[39m, \u001b[34mNumericRange{Float64,…} @359\u001b[39m, \u001b[34mNominalRange{Bool,…} @827\u001b[39m, \u001b[34mNominalRange{Bool,…} @969\u001b[39m],\n",
       "            selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "            train_best = true,\n",
       "            repeats = 5,\n",
       "            n = nothing,\n",
       "            acceleration = CPU1{Nothing}(nothing),\n",
       "            acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "            check_measure = true,\n",
       "            cache = true))\u001b[34m @397\u001b[39m"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = @pipeline pca model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @776\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @925\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @016\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @776\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 1200 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 1200 metamodels: 100%[=========================] Time: 0:00:41\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @776\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @925\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @016\u001b[39m ⏎ `AbstractVector{Multiclass{2}}`\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(predict(mach, X[test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21722846441947566"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticClassifier(\n",
       "    lambda = 3.4,\n",
       "    gamma = 6.666666666666667,\n",
       "    penalty = :l2,\n",
       "    fit_intercept = true,\n",
       "    penalize_intercept = true,\n",
       "    solver = nothing)\u001b[34m @244\u001b[39m"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach).best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "kaggle score: 0.78947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLIBSVMInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(\n",
       "    kernel = LIBSVM.Kernel.RadialBasis,\n",
       "    gamma = 0.0,\n",
       "    weights = nothing,\n",
       "    cost = 1.0,\n",
       "    cachesize = 200.0,\n",
       "    degree = 3,\n",
       "    coef0 = 0.0,\n",
       "    tolerance = 0.001,\n",
       "    shrinking = true,\n",
       "    probability = false)\u001b[34m @655\u001b[39m"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline429(\n",
       "    standardizer = Standardizer(\n",
       "            features = Symbol[],\n",
       "            ignore = false,\n",
       "            ordered_factor = false,\n",
       "            count = false),\n",
       "    svc = SVC(\n",
       "            kernel = LIBSVM.Kernel.RadialBasis,\n",
       "            gamma = 0.0,\n",
       "            weights = nothing,\n",
       "            cost = 1.0,\n",
       "            cachesize = 200.0,\n",
       "            degree = 3,\n",
       "            coef0 = 0.0,\n",
       "            tolerance = 0.001,\n",
       "            shrinking = true,\n",
       "            probability = false))\u001b[34m @269\u001b[39m"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = @pipeline stand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{Pipeline429,…} @057\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @866\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @624\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{Pipeline429,…} @057\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Training \u001b[34mMachine{Standardizer,…} @556\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Training \u001b[34mMachine{SVC,…} @060\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{Pipeline429,…} @057\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @866\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @624\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, X[test, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17228464419475656"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest\n",
    "kaggle score: 0.77033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJDecisionTreeInterface.RandomForestClassifier"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = @load RandomForestClassifier pkg=\"DecisionTree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = -1,\n",
       "    n_trees = 10,\n",
       "    sampling_fraction = 0.7,\n",
       "    pdf_smoothing = 0.0)\u001b[34m @972\u001b[39m"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(model, :max_depth, lower=-1, upper=5)\n",
    "r2 = range(model, :min_samples_leaf, lower=1, upper=20)\n",
    "r3 = range(model, :min_samples_split, lower=1, upper=40)\n",
    "r4 = range(model, :n_subfeatures, lower=0, upper=5)\n",
    "r5 = range(model, :n_trees, lower=2, upper=15)\n",
    "r6 = range(model, :sampling_fraction, lower=0.5, upper=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = RandomForestClassifier(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 1,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = -1,\n",
       "            n_trees = 10,\n",
       "            sampling_fraction = 0.7,\n",
       "            pdf_smoothing = 0.0),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = Holdout(\n",
       "            fraction_train = 0.8,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = LogLoss(\n",
       "            tol = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[\u001b[34mNumericRange{Int64,…} @562\u001b[39m, \u001b[34mNumericRange{Int64,…} @286\u001b[39m, \u001b[34mNumericRange{Int64,…} @121\u001b[39m, \u001b[34mNumericRange{Int64,…} @141\u001b[39m, \u001b[34mNumericRange{Int64,…} @883\u001b[39m, \u001b[34mNumericRange{Float64,…} @909\u001b[39m],\n",
       "    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "    train_best = true,\n",
       "    repeats = 2,\n",
       "    n = 100000,\n",
       "    acceleration = CPUProcesses{Nothing}(nothing),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true,\n",
       "    cache = true)\u001b[34m @233\u001b[39m"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TunedModel(model=model,\n",
    "                  ranges=[r, r2, r3, r4, r5, r6],\n",
    "                  resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                  measures=cross_entropy,\n",
    "                  n=100000,\n",
    "                  repeats=2,\n",
    "                  acceleration=CPUProcesses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @258\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @566\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @967\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @258\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 100000 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 100000 metamodels: 100%[=========================] Time: 0:05:03\u001b[39mm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @258\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @566\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @967\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(predict(mach, X[test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18726591760299627"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(\n",
       "    max_depth = 3,\n",
       "    min_samples_leaf = 12,\n",
       "    min_samples_split = 23,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 1,\n",
       "    n_trees = 12,\n",
       "    sampling_fraction = 0.6666666666666666,\n",
       "    pdf_smoothing = 0.0)\u001b[34m @431\u001b[39m"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach).best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"random_tree.jlso\", mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @975\u001b[39m trained 1 time; caches data\n",
       "  args: \n"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mach = machine(\"random_tree.jlso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: intensive tuning and write down the machine\n",
    "# For NN also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier\n",
    "vanilla kaggle score: 0.77751\n",
    "\n",
    "after some tuning: 0.78468, 0.77990, 0.77751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import LightGBM ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/ahautelman/.julia/packages/MLJModels/zYlo3/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBM.MLJInterface.LGBMClassifier"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = @load LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(\n",
       "    boosting = \"dart\",\n",
       "    num_iterations = 10,\n",
       "    learning_rate = 0.1,\n",
       "    num_leaves = 31,\n",
       "    max_depth = -1,\n",
       "    tree_learner = \"serial\",\n",
       "    histogram_pool_size = -1.0,\n",
       "    min_data_in_leaf = 20,\n",
       "    min_sum_hessian_in_leaf = 0.001,\n",
       "    max_delta_step = 0.0,\n",
       "    lambda_l1 = 0.0,\n",
       "    lambda_l2 = 0.0,\n",
       "    min_gain_to_split = 0.0,\n",
       "    feature_fraction = 1.0,\n",
       "    feature_fraction_bynode = 1.0,\n",
       "    feature_fraction_seed = 2,\n",
       "    bagging_fraction = 1.0,\n",
       "    pos_bagging_fraction = 1.0,\n",
       "    neg_bagging_fraction = 1.0,\n",
       "    bagging_freq = 0,\n",
       "    bagging_seed = 3,\n",
       "    early_stopping_round = 0,\n",
       "    extra_trees = false,\n",
       "    extra_seed = 6,\n",
       "    max_bin = 255,\n",
       "    bin_construct_sample_cnt = 200000,\n",
       "    init_score = \"\",\n",
       "    drop_rate = 0.1,\n",
       "    max_drop = 50,\n",
       "    skip_drop = 0.5,\n",
       "    xgboost_dart_mode = false,\n",
       "    uniform_drop = false,\n",
       "    drop_seed = 4,\n",
       "    top_rate = 0.2,\n",
       "    other_rate = 0.1,\n",
       "    min_data_per_group = 100,\n",
       "    max_cat_threshold = 32,\n",
       "    cat_l2 = 10.0,\n",
       "    cat_smooth = 10.0,\n",
       "    objective = \"multiclass\",\n",
       "    categorical_feature = Int64[],\n",
       "    data_random_seed = 1,\n",
       "    is_sparse = false,\n",
       "    is_unbalance = false,\n",
       "    boost_from_average = true,\n",
       "    scale_pos_weight = 1.0,\n",
       "    use_missing = true,\n",
       "    feature_pre_filter = true,\n",
       "    metric = [\"None\"],\n",
       "    metric_freq = 1,\n",
       "    is_training_metric = false,\n",
       "    ndcg_at = [1, 2, 3, 4, 5],\n",
       "    num_machines = 1,\n",
       "    num_threads = 0,\n",
       "    local_listen_port = 12400,\n",
       "    time_out = 120,\n",
       "    machine_list_file = \"\",\n",
       "    save_binary = false,\n",
       "    device_type = \"cpu\",\n",
       "    force_col_wise = false,\n",
       "    force_row_wise = false,\n",
       "    truncate_booster = true)\u001b[34m @649\u001b[39m"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(boosting=\"dart\", is_sparse=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NumericRange)(Float64, :min_gain_to_split, ... )"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(model, :max_bin, lower=20, upper=255)\n",
    "# r2 = range(model, :num_leaves, lower=5, upper=40)\n",
    "r3 = range(model, :min_data_in_leaf, lower=5, upper=30)\n",
    "# r4 = range(model, :bagging_freq, lower=0, upper=10)\n",
    "r5 = range(model, :bagging_fraction, lower=0.5, upper=1.0)\n",
    "r6 = range(model, :feature_fraction, lower=0.5, upper=1.0)\n",
    "# r7 = range(model, :lambda_l1, lower=0.0, upper=0.3)\n",
    "r8 = range(model, :lambda_l2, lower=0.0, upper=0.3)\n",
    "r9 = range(model, :min_gain_to_split, lower=0.0, upper=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = LGBMClassifier(\n",
       "            boosting = \"dart\",\n",
       "            num_iterations = 10,\n",
       "            learning_rate = 0.1,\n",
       "            num_leaves = 31,\n",
       "            max_depth = -1,\n",
       "            tree_learner = \"serial\",\n",
       "            histogram_pool_size = -1.0,\n",
       "            min_data_in_leaf = 20,\n",
       "            min_sum_hessian_in_leaf = 0.001,\n",
       "            max_delta_step = 0.0,\n",
       "            lambda_l1 = 0.0,\n",
       "            lambda_l2 = 0.0,\n",
       "            min_gain_to_split = 0.0,\n",
       "            feature_fraction = 1.0,\n",
       "            feature_fraction_bynode = 1.0,\n",
       "            feature_fraction_seed = 2,\n",
       "            bagging_fraction = 1.0,\n",
       "            pos_bagging_fraction = 1.0,\n",
       "            neg_bagging_fraction = 1.0,\n",
       "            bagging_freq = 0,\n",
       "            bagging_seed = 3,\n",
       "            early_stopping_round = 0,\n",
       "            extra_trees = false,\n",
       "            extra_seed = 6,\n",
       "            max_bin = 255,\n",
       "            bin_construct_sample_cnt = 200000,\n",
       "            init_score = \"\",\n",
       "            drop_rate = 0.1,\n",
       "            max_drop = 50,\n",
       "            skip_drop = 0.5,\n",
       "            xgboost_dart_mode = false,\n",
       "            uniform_drop = false,\n",
       "            drop_seed = 4,\n",
       "            top_rate = 0.2,\n",
       "            other_rate = 0.1,\n",
       "            min_data_per_group = 100,\n",
       "            max_cat_threshold = 32,\n",
       "            cat_l2 = 10.0,\n",
       "            cat_smooth = 10.0,\n",
       "            objective = \"multiclass\",\n",
       "            categorical_feature = Int64[],\n",
       "            data_random_seed = 1,\n",
       "            is_sparse = false,\n",
       "            is_unbalance = false,\n",
       "            boost_from_average = true,\n",
       "            scale_pos_weight = 1.0,\n",
       "            use_missing = true,\n",
       "            feature_pre_filter = true,\n",
       "            metric = [\"None\"],\n",
       "            metric_freq = 1,\n",
       "            is_training_metric = false,\n",
       "            ndcg_at = [1, 2, 3, 4, 5],\n",
       "            num_machines = 1,\n",
       "            num_threads = 0,\n",
       "            local_listen_port = 12400,\n",
       "            time_out = 120,\n",
       "            machine_list_file = \"\",\n",
       "            save_binary = false,\n",
       "            device_type = \"cpu\",\n",
       "            force_col_wise = false,\n",
       "            force_row_wise = false,\n",
       "            truncate_booster = true),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = Holdout(\n",
       "            fraction_train = 0.8,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = LogLoss(\n",
       "            tol = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[\u001b[34mNumericRange{Int64,…} @362\u001b[39m, \u001b[34mNumericRange{Int64,…} @701\u001b[39m, \u001b[34mNumericRange{Float64,…} @107\u001b[39m, \u001b[34mNumericRange{Float64,…} @516\u001b[39m, \u001b[34mNumericRange{Float64,…} @193\u001b[39m, \u001b[34mNumericRange{Float64,…} @543\u001b[39m],\n",
       "    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "    train_best = true,\n",
       "    repeats = 2,\n",
       "    n = 10000,\n",
       "    acceleration = CPU1{Nothing}(nothing),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true,\n",
       "    cache = true)\u001b[34m @059\u001b[39m"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TunedModel(model=model,\n",
    "                   ranges=[r, r3, r5, r6, r9, r8],\n",
    "                   resampling=Holdout(fraction_train=0.8, shuffle=true),\n",
    "                   measures=cross_entropy,\n",
    "                   n=10000,\n",
    "                   repeats=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @688\u001b[39m trained 0 times; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @278\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @882\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @688\u001b[39m.\n",
      "└ @ MLJBase /home/ahautelman/.julia/packages/MLJBase/KWyqX/src/machines.jl:342\n",
      "┌ Info: Attempting to evaluate 10000 models.\n",
      "└ @ MLJTuning /home/ahautelman/.julia/packages/MLJTuning/9sSuR/src/tuned_models.jl:564\n",
      "\u001b[33mEvaluating over 10000 metamodels: 100%[=========================] Time: 0:05:07\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…},…} @688\u001b[39m trained 1 time; caches data\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @278\u001b[39m ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\t\u001b[34mSource @882\u001b[39m ⏎ `AbstractVector{OrderedFactor{2}}`\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = mode.(MLJ.predict(mach, X[test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1647940074906367"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_rate(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(\n",
       "    boosting = \"dart\",\n",
       "    num_iterations = 10,\n",
       "    learning_rate = 0.1,\n",
       "    num_leaves = 31,\n",
       "    max_depth = -1,\n",
       "    tree_learner = \"serial\",\n",
       "    histogram_pool_size = -1.0,\n",
       "    min_data_in_leaf = 8,\n",
       "    min_sum_hessian_in_leaf = 0.001,\n",
       "    max_delta_step = 0.0,\n",
       "    lambda_l1 = 0.0,\n",
       "    lambda_l2 = 0.16666666666666666,\n",
       "    min_gain_to_split = 0.06666666666666667,\n",
       "    feature_fraction = 0.9444444444444444,\n",
       "    feature_fraction_bynode = 1.0,\n",
       "    feature_fraction_seed = 2,\n",
       "    bagging_fraction = 1.0,\n",
       "    pos_bagging_fraction = 1.0,\n",
       "    neg_bagging_fraction = 1.0,\n",
       "    bagging_freq = 0,\n",
       "    bagging_seed = 3,\n",
       "    early_stopping_round = 0,\n",
       "    extra_trees = false,\n",
       "    extra_seed = 6,\n",
       "    max_bin = 229,\n",
       "    bin_construct_sample_cnt = 200000,\n",
       "    init_score = \"\",\n",
       "    drop_rate = 0.1,\n",
       "    max_drop = 50,\n",
       "    skip_drop = 0.5,\n",
       "    xgboost_dart_mode = false,\n",
       "    uniform_drop = false,\n",
       "    drop_seed = 4,\n",
       "    top_rate = 0.2,\n",
       "    other_rate = 0.1,\n",
       "    min_data_per_group = 100,\n",
       "    max_cat_threshold = 32,\n",
       "    cat_l2 = 10.0,\n",
       "    cat_smooth = 10.0,\n",
       "    objective = \"multiclass\",\n",
       "    categorical_feature = Int64[],\n",
       "    data_random_seed = 1,\n",
       "    is_sparse = false,\n",
       "    is_unbalance = false,\n",
       "    boost_from_average = true,\n",
       "    scale_pos_weight = 1.0,\n",
       "    use_missing = true,\n",
       "    feature_pre_filter = true,\n",
       "    metric = [\"None\"],\n",
       "    metric_freq = 1,\n",
       "    is_training_metric = false,\n",
       "    ndcg_at = [1, 2, 3, 4, 5],\n",
       "    num_machines = 1,\n",
       "    num_threads = 0,\n",
       "    local_listen_port = 12400,\n",
       "    time_out = 120,\n",
       "    machine_list_file = \"\",\n",
       "    save_binary = false,\n",
       "    device_type = \"cpu\",\n",
       "    force_col_wise = false,\n",
       "    force_row_wise = false,\n",
       "    truncate_booster = true)\u001b[34m @157\u001b[39m"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach).best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"LGBM777.jlso\", mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mode.(MLJ.predict(mach, x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(predictions, \"data/light_GBM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_to_file (generic function with 1 method)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function write_to_file(data, path::String)\n",
    "    fw = open(path, \"w\")\n",
    "    write(fw, \"PassengerId,Survived\\n\")\n",
    "    close(fw)\n",
    "    fw = open(path, \"a\")\n",
    "    index = 892\n",
    "    for value in data\n",
    "        write(fw, \"$index,$value\\n\")\n",
    "        index += 1\n",
    "    end\n",
    "    close(fw)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
